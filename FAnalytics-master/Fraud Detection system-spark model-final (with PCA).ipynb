{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:40:46.355575Z",
     "start_time": "2019-04-10T09:40:46.337528Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:40:50.821447Z",
     "start_time": "2019-04-10T09:40:46.359586Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:00.221770Z",
     "start_time": "2019-04-10T09:40:50.824457Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:11.775203Z",
     "start_time": "2019-04-10T09:41:00.227785Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data = spark.read.csv(\"New_Aggregated_data_final.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:12.426938Z",
     "start_time": "2019-04-10T09:41:11.779215Z"
    }
   },
   "outputs": [],
   "source": [
    "customer_data = spark.read.csv(\"Customer_data1.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:12.766840Z",
     "start_time": "2019-04-10T09:41:12.431951Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data = trans_data.withColumn(\"sum_prev_day_onl\", trans_data[\"sum_prev_day_onl\"].cast(\"integer\"))\n",
    "trans_data = trans_data.withColumn(\"sum_prev_day_mon_onl\", trans_data[\"sum_prev_day_mon_onl\"].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:13.055609Z",
     "start_time": "2019-04-10T09:41:12.771854Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data1 = trans_data.select('_c0','amt', 'Balance',\n",
    "      'sum_prev_day', 'cnt_prev_day_onl', 'sum_prev_day_onl',\n",
    "       '24hrsAvg','qtrAvg','wkAvg', 'monAvg','yrAvg').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:13.063629Z",
     "start_time": "2019-04-10T09:41:13.058616Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# train.where(col('cc_num').isNull()).count()\n",
    "# df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "# customer_data.select([count(when(col(c).isNull(),c)).alias(c) for c in customer_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:13.095715Z",
     "start_time": "2019-04-10T09:41:13.067640Z"
    }
   },
   "outputs": [],
   "source": [
    "# # merging the data together by their unique \"id\"\n",
    "# train = trans_data.join(customer_data,how='left',on='cc_num')\n",
    "# # all_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:13.212030Z",
     "start_time": "2019-04-10T09:41:13.098721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amt',\n",
       " 'Balance',\n",
       " 'sum_prev_day',\n",
       " 'cnt_prev_day_onl',\n",
       " 'sum_prev_day_onl',\n",
       " '24hrsAvg',\n",
       " 'qtrAvg',\n",
       " 'wkAvg',\n",
       " 'monAvg',\n",
       " 'yrAvg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a single vector column\n",
    "cols = trans_data1.drop('_c0').columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:14.732953Z",
     "start_time": "2019-04-10T09:41:13.215042Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5c9fa14e9e1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#apply PCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pcaFeatures'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaledData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtransformed_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaledData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "#apply PCA\n",
    "pca = PCA(k=8, inputCol=scaler.getOutputCol(), outputCol='pcaFeatures')\n",
    "\n",
    "model = pca.fit(scaledData)\n",
    "transformed_feature = model.transform(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:45.523951Z",
     "start_time": "2019-04-10T09:41:45.503898Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6dc7ab8e47fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "scaler.getOutputCol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:45.525955Z",
     "start_time": "2019-04-10T09:41:45.503Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "# pca_features = scaledData.select(\"scaledFeatures\").rdd.map(lambda row : row[0])\n",
    "# mat = RowMatrix(pca_features)\n",
    "# svd = mat.computeSVD(5,True)\n",
    "# svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:45.527961Z",
     "start_time": "2019-04-10T09:41:45.507Z"
    }
   },
   "outputs": [],
   "source": [
    "# percentage of variance explained by each PC\n",
    "np.round(100.00*model.explainedVariance.toArray(),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:45.529967Z",
     "start_time": "2019-04-10T09:41:45.511Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute loadings of each feature\n",
    "pcs = np.round(model.pc.toArray(),4)\n",
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:45.531972Z",
     "start_time": "2019-04-10T09:41:45.515Z"
    }
   },
   "outputs": [],
   "source": [
    "pcs = np.round(model.pc.toArray(),4)\n",
    "df_pc = pd.DataFrame(pcs, columns = ['PC'+str(i) for i in range(1, 9)], index = cols)\n",
    "df_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:45.533978Z",
     "start_time": "2019-04-10T09:41:45.519Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pc['PC1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:43:49.958371Z",
     "start_time": "2019-04-10T09:43:49.763687Z"
    },
    "code_folding": [
     27
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.context import HiveContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler,PCA,StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.param import Param,Params\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.common import callMLlibFunc, JavaModelWrapper\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.mllib.linalg import _convert_to_vector, Vectors, Matrix, DenseMatrix\n",
    "from pyspark.sql.functions import array, col, explode, struct, lit, udf, sum, when,avg,pow,sqrt,mean,log,desc\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import time, os, sys, json, math\n",
    "import datetime as dt\n",
    "import subprocess\n",
    "import getpass\n",
    "import pdb\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from numpy.linalg import eigh\n",
    "import itertools\n",
    "\n",
    "class princomp:\n",
    "    def __init__(self,n=5,std=True,prefix='pcomp'):\n",
    "        self.n=n\n",
    "        self.std=std\n",
    "        self.prefix=prefix\n",
    "        self.cols = [self.prefix+str(i) for i in list(range(1,self.n+1))]\n",
    "        self.inputcol = \"std_features\"\n",
    "        self.outputcol = \"pca_features\"\n",
    "        self.vars = 'all'\n",
    "        self.model = PCA(k=self.n,inputCol=self.inputcol,outputCol=self.outputcol)\n",
    "        self.components = spark.createDataFrame(pd.DataFrame([0]))\n",
    "        self.projections = spark.createDataFrame(pd.DataFrame([0]))\n",
    "        self.result = spark.createDataFrame(pd.DataFrame([0]))\n",
    "    def helpme(self):\n",
    "        print ('|-- Please note that output_parameters will have null values before calling the fit method')\n",
    "        print ('|-- n : input_parameter : sets the number of principal components, default = 5')\n",
    "        print ('|-- std : input_parameter : True/False value specifying whether to standardize the principal components, default = True')\n",
    "        print ('|-- prefix : input_parameter : string specifying the prefix for columns of principal components , default = pcomp')\n",
    "        print ('|-- vars : input_parameter : list of variable names to be used for principal components, default = all')\n",
    "        print ('|-- components : output_parameter : pandas dataframe of coefficients of different input columns for computing principal components')\n",
    "        print ('|-- result : output_parameter : spark dataframe of original variables joined with projections of principal components')\n",
    "        print ('|-- covariance : output_parameter : Numpy array of the covariance matrix')\n",
    "        print ('|-- eigvals : output_parameter : Numpy array of all eigenvalues')\n",
    "        print ('|-- eigvecs : output_parameter : Numpy array of all eigenvectors')\n",
    "        print ('|-- varianceexplained : output_parameter : variance explained by the n principal components')\n",
    "        print ('|-- outputcompfile(file) : method : outputs the components matrix to the specified file')\n",
    "        print ('|-- fit(inputdf,myfeatures) : method : fit method which computes all output parameters')\n",
    "    # SET methods\n",
    "    def setn(self,val):\n",
    "        self.n = val\n",
    "        self.cols = [self.prefix+str(i) for i in list(range(1,self.n+1))]\n",
    "        self.model = PCA(k=self.n,inputCol=self.inputcol,outputCol=self.outputcol)\n",
    "    def setstd(self,val):\n",
    "        self.std = val\n",
    "    def setprefix(self,val):\n",
    "        self.prefix=val\n",
    "        self.cols = [self.prefix+str(i) for i in list(range(1,self.n+1))]\n",
    "    def setcols(self,val):\n",
    "        self.cols = val\n",
    "    def setinputcol(self,val):\n",
    "        self.inputcol = val\n",
    "        self.model = PCA(k=self.n,inputCol=self.inputcol,outputCol=self.outputcol)\n",
    "    def setoutputcol(self,val):\n",
    "        self.outputcol = val\n",
    "        self.model = PCA(k=self.n,inputCol=self.inputcol,outputCol=self.outputcol)\n",
    "    def setvars(self,val):\n",
    "        self.vars = val\n",
    "    def setmodel(self,val):\n",
    "        self.model = val\n",
    "    def setcomponents(self,val):\n",
    "        self.components=val\n",
    "    def setprojections(self,val):\n",
    "        self.projections=val\n",
    "    def setresult(self,val):\n",
    "        self.result = val\n",
    "    # GET methods\n",
    "    def getn(self):\n",
    "        return self.n\n",
    "    def getstd(self):\n",
    "        return self.std\n",
    "    def getprefix(self):\n",
    "        return self.prefix\n",
    "    def getcols(self):\n",
    "        return self.cols\n",
    "    def getinputcol(self):\n",
    "        return self.inputcol\n",
    "    def getoutputcol(self):\n",
    "        return self.outputcol\n",
    "    def getvars(self):\n",
    "        return self.vars\n",
    "    def getmodel(self):\n",
    "        return self.model\n",
    "    def getcomponents(self):\n",
    "        return self.components\n",
    "    def getprojections(self):\n",
    "        return self.projections\n",
    "    def getresult(self):\n",
    "        return self.result\n",
    "    # CORE methods\n",
    "    def vectorizedf(self,inputdf,vars='all'):\n",
    "        \"\"\"Returns the input spark dataframe with an additional column of dense vector features\"\"\"\n",
    "        if vars=='all':\n",
    "            myfeatures = inputdf.columns\n",
    "        else:\n",
    "            myfeatures=vars\n",
    "\n",
    "        assembler = VectorAssembler(inputCols = myfeatures,outputCol=\"features\")\n",
    "        assembled = assembler.transform(inputdf)\n",
    "\n",
    "        as_dense = udf(\n",
    "            lambda v: DenseVector(v.toArray()) if v is not None else None,\n",
    "            VectorUDT()\n",
    "        )\n",
    "\n",
    "        df_dense = assembled.withColumn(\"features1\", as_dense(assembled.features))\n",
    "        df_dense2 = df_dense.drop(\"features\")\n",
    "        df_dense3 = df_dense2.withColumnRenamed(\"features1\",\"features\")\n",
    "        return df_dense3\n",
    "    def outputcompfile(self,filewlocation):\n",
    "        \"\"\" Outputs the loading of principal components to the file specified\"\"\"\n",
    "        df = self.components\n",
    "        df.to_csv(filewlocation,index=False)\n",
    "        print (\"Component matrix is now available at the location : \"+filewlocation)\n",
    "    def identity(self):\n",
    "        \"\"\" Outputs an identity matrix in the form of features column in a dataframe\"\"\"\n",
    "        iden = np.identity(len(self.vars)).tolist()\n",
    "        rddi = sc.parallelize(iden)\n",
    "        df_identity = rddi.map(lambda line:Row(std_features=Vectors.dense(line))).toDF()\n",
    "        return df_identity\n",
    "    def fit(self,inputdf,myfeatures):\n",
    "        \"\"\" Fits the input dataframe in a PCA model with the given features \"\"\"\n",
    "        start_time = time.time()      # Start Timer\n",
    "        if myfeatures=='all':\n",
    "            self.vars = inputdf.columns\n",
    "        else:\n",
    "            self.vars = myfeatures\n",
    "        # vectorize and scale\n",
    "        df_dense = self.vectorizedf(inputdf,self.vars)\n",
    "        df_normalized = self.scalemeanstd(df_dense)\n",
    "        # Compute covariance matrix, eigenvalues and eigenvectors\n",
    "        dfzeromean = df_normalized.select(self.inputcol)\n",
    "        self.covariance = dfzeromean.map(lambda x:np.outer(x,x)).sum()/dfzeromean.count()\n",
    "        col1 = self.covariance.shape[1]\n",
    "        eigvals,eigvecs = eigh(self.covariance)\n",
    "        inds = np.argsort(eigvals)\n",
    "        self.eigvals = eigvals[inds[-1:-(col1+1):-1]]\n",
    "        self.eigvecs = -1*eigvecs.T[inds[-1:-(col1+1):-1]]\n",
    "        self.varianceexplained = np.sum(self.eigvals[0:self.n])/np.sum(self.eigvals)\n",
    "        # Fit PCA model\n",
    "        model1 = self.model.fit(df_normalized)\n",
    "        df_features = model1.transform(df_normalized)\n",
    "        # Compute components and put in a pandas dataframe\n",
    "        df_identity = self.identity()\n",
    "        components = model1.transform(df_identity)\n",
    "        components = components.withColumnRenamed('pca_features','components')\n",
    "        edf_rdd = components.select(\"components\").rdd.map(lambda x: tuple(x.components.toArray().tolist()))\n",
    "        edf_pandas = edf_rdd.toDF(self.cols).toPandas()\n",
    "        comp_ind = sqlContext.createDataFrame([Row(industries=self.vars)])\n",
    "        comp_ind_pandas = comp_ind.select(explode(comp_ind.industries).alias(\"Variable\")).toPandas()\n",
    "        self.components = pd.concat([comp_ind_pandas,edf_pandas],axis=1)\n",
    "        # Compute and standardize projections if self.std = True\n",
    "        if self.std:\n",
    "            projections1=self.scalemeanstd(df_features,inputcol = \"pca_features\",outputcol = \"projections\")\n",
    "        else:\n",
    "            projections1 = df_features.withColumnRenamed('pca_features','projections')\n",
    "        # Prepare data for output\n",
    "        self.projections = projections1.select('projections')\n",
    "        drop_list = ['features','std_features','pca_features']\n",
    "        projections2 = projections1.select([c for c in projections1.columns if c not in drop_list])\n",
    "        l = ['x.'+c for c in inputdf.columns]\n",
    "        cst = '['+\",\".join(l)+']'\n",
    "        final_df = projections1.rdd.map(lambda x: tuple(eval(cst))+tuple(x.projections.toArray().tolist())).toDF(inputdf.columns+self.cols)\n",
    "        self.result = final_df\n",
    "        print(\"PCA fitting took a total of %s seconds \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:45.553028Z",
     "start_time": "2019-04-10T09:41:45.531Z"
    }
   },
   "outputs": [],
   "source": [
    "fs = princomp(n=15)\n",
    "fs.fit(trans_data1,cols) # here select_features is the list of columns that you wish to perform PCA on\n",
    "fs.outputcompfile('components.csv')\n",
    "print (fs.eigvals)\n",
    "print (fs.varianceexplained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:41:45.569072Z",
     "start_time": "2019-04-10T09:41:45.538Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#  # Compute covariance matrix, eigenvalues and eigenvectors\n",
    "dfzeromean = scaledData.select('scaledFeatures')\n",
    "# cov = as_cov(assembled.features)\n",
    "\n",
    "dfzeromean.map(lambda x:np.outer(x,x)).sum()/dfzeromean.count()\n",
    "\n",
    "as_cov = udf(\n",
    "            lambda x:np.outer(x,x)\n",
    "        )\n",
    "\n",
    "# # col1 = self.covariance.shape[1]\n",
    "# # eigvals,eigvecs = eigh(self.covariance)\n",
    "# # inds = np.argsort(eigvals)\n",
    "# # self.eigvals = eigvals[inds[-1:-(col1+1):-1]]\n",
    "# # self.eigvecs = -1*eigvecs.T[inds[-1:-(col1+1):-1]]\n",
    "# # self.varianceexplained = np.sum(self.eigvals[0:self.n])/np.sum(self.eigvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:43:22.432621Z",
     "start_time": "2019-04-10T09:43:19.325502Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------+\n",
      "|_c0|features                                                     |\n",
      "+---+-------------------------------------------------------------+\n",
      "|0  |(10,[0,1],[95.0,942.0])                                      |\n",
      "|1  |(10,[0],[90.0])                                              |\n",
      "|2  |(10,[0,1],[188.0,6746.0])                                    |\n",
      "|3  |[100.0,133.0,373.0,0.0,0.0,124.33,124.33,124.33,124.33,124.0]|\n",
      "|4  |[79.0,3115.0,100.0,0.0,0.0,100.0,118.25,118.25,118.25,118.0] |\n",
      "+---+-------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0|scaledFeatures                                                                                                                                                                                                  |\n",
      "+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |[-0.15324509141918746,-0.20824171850384487,-0.733277853019407,-0.3436387790372,-0.21640155213233406,-0.6804918641670578,-1.2617558227807768,-1.1505750091879985,-1.2597350985596205,-1.2583192791022988]        |\n",
      "|1  |[-0.17600379267423188,-0.49825834867770424,-0.733277853019407,-0.3436387790372,-0.21640155213233406,-0.6804918641670578,-1.2617558227807768,-1.1505750091879985,-1.2597350985596205,-1.2583192791022988]        |\n",
      "|2  |[0.2700667519246385,1.578654801166091,-0.733277853019407,-0.3436387790372,-0.21640155213233406,-0.6804918641670578,-1.2617558227807768,-1.1505750091879985,-1.2597350985596205,-1.2583192791022988]             |\n",
      "|3  |[-0.13048639016414307,-0.45731120237927186,-0.2293178275752496,-0.3436387790372,-0.21640155213233406,0.025241540379388604,-0.035226090095605574,-0.017476879756091353,-0.0416108139898843,-0.034692009461999754]|\n",
      "|4  |[-0.22607293543532958,0.4607669198908434,-0.5981679266268715,-0.3436387790372,-0.21640155213233406,-0.11286264793087471,-0.09520598854912314,-0.07288777517108366,-0.10117966825019149,-0.09389978057362712]    |\n",
      "+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "output_dat = assembler.transform(trans_data1).select('_c0','features')\n",
    "output_dat.show(5, truncate = False)\n",
    "\n",
    "#Center and scale data\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(output_dat)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(output_dat)\n",
    "scaledData.select(['_c0','scaledFeatures']).show(5, truncate=False) #sample centered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:42:21.537626Z",
     "start_time": "2019-04-10T09:42:21.531610Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def vectorizedf(inputdf,vars='all'):\n",
    "        \"\"\"Returns the input spark dataframe with an additional column of dense vector features\"\"\"\n",
    "#         if vars=='all':\n",
    "#             myfeatures = inputdf.columns\n",
    "#         else:\n",
    "#             myfeatures=vars\n",
    "\n",
    "#         assembler = VectorAssembler(inputCols = myfeatures,outputCol=\"features\")\n",
    "#         assembled = assembler.transform(inputdf)\n",
    "\n",
    "        as_dense = udf(\n",
    "            lambda v: DenseVector(v.toArray()) if v is not None else None,\n",
    "            VectorUDT()\n",
    "        )\n",
    "\n",
    "        df_dense = inputdf.withColumn(\"features\", as_dense(inputdf.scaledFeatures))\n",
    "#         df_dense2 = df_dense.drop(\"features\")\n",
    "#         df_dense3 = df_dense2.withColumnRenamed(\"features1\",\"features\")\n",
    "        return df_dense\n",
    "\n",
    "def estimateCovariance(df):\n",
    "    \"\"\"Compute the covariance matrix for a given dataframe.\n",
    "\n",
    "    Note:\n",
    "        The multi-dimensional covariance array should be calculated using outer products.  Don't\n",
    "        forget to normalize the data by first subtracting the mean.\n",
    "\n",
    "    Args:\n",
    "        df:  A Spark dataframe with a column named 'features', which (column) consists of DenseVectors.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the\n",
    "            length of the arrays in the input dataframe.\n",
    "    \"\"\"\n",
    "#     m = df.select(df['features']).rdd.map(lambda x: x[0]).mean()\n",
    "#     dfZeroMean = df.select(df['features']).rdd.map(lambda x:   x[0]).map(lambda x: x-m)  # subtract the mean\n",
    "    dfZeroMean = df.select('scaledFeatures')\n",
    "\n",
    "    return dfZeroMean.rdd.map(lambda x: np.outer(x,x)).sum()/df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:42:21.570713Z",
     "start_time": "2019-04-10T09:42:21.541637Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import eigh\n",
    "\n",
    "def pca(df, k=2):\n",
    "    \"\"\"Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n",
    "\n",
    "    Note:\n",
    "        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
    "        each eigenvectors as a column.  This function should also return eigenvectors as columns.\n",
    "\n",
    "    Args:\n",
    "        df: A Spark dataframe with a 'features' column, which (column) consists of DenseVectors.\n",
    "        k (int): The number of principal components to return.\n",
    "\n",
    "    Returns:\n",
    "        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of\n",
    "        scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of\n",
    "        rows equals the length of the arrays in the input `RDD` and the number of columns equals\n",
    "        `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays\n",
    "        of length `k`.  Eigenvalues is an array of length d (the number of features).\n",
    "     \"\"\"\n",
    "    dfT = vectorizedf(df)\n",
    "    cov = estimateCovariance(dfT)\n",
    "    col = cov.shape[1]\n",
    "    eigVals, eigVecs = eigh(cov)\n",
    "    inds = np.argsort(eigVals)\n",
    "    eigVecs = eigVecs.T[inds[-1:-(col+1):-1]]  \n",
    "    components = eigVecs[0:k]\n",
    "    eigVals = eigVals[inds[-1:-(col+1):-1]]  # sort eigenvals\n",
    "    score = \"df.select(dfT['features']).rdd.map(lambda x: x[0]).map(lambda x: np.dot(x, components.T) )\"\n",
    "    # Return the `k` principal components, `k` scores, and all eigenvalues\n",
    "\n",
    "    return components.T, score, eigVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:42:21.756207Z",
     "start_time": "2019-04-10T09:42:21.574725Z"
    }
   },
   "outputs": [],
   "source": [
    " data = [(Vectors.dense([0.0, 1.0, 0.0, 7.0, 0.0]),),\n",
    "...     (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "...     (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    ">>> df = spark.createDataFrame(data,[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:42:21.792304Z",
     "start_time": "2019-04-10T09:42:21.760220Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c1ec4d3773db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# [Row(pca_features=DenseVector([1.6486, 4.0133])),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#  Row(pca_features=DenseVector([-4.6451, 1.1168])),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#  Row(pca_features=DenseVector([-6.4289, 5.338]))]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    ">>> score.collect()\n",
    "# [Row(pca_features=DenseVector([1.6486, 4.0133])),\n",
    "#  Row(pca_features=DenseVector([-4.6451, 1.1168])),\n",
    "#  Row(pca_features=DenseVector([-6.4289, 5.338]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T10:04:35.377060Z",
     "start_time": "2019-04-10T10:04:35.371076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.999525211281034"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigVals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T10:04:31.466573Z",
     "start_time": "2019-04-10T10:04:24.484008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.37619724e+00, 1.50251168e+00, 9.99652500e-01, 9.01024936e-01,\n",
       "       6.34460694e-01, 2.24052375e-01, 1.80304562e-01, 1.70305810e-01,\n",
       "       1.02872247e-02, 7.28187348e-04])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp, score, eigVals = pca(scaledData)\n",
    "eigVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:44:14.068250Z",
     "start_time": "2019-04-10T09:44:14.065243Z"
    }
   },
   "outputs": [],
   "source": [
    "eigVals /= eigVals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T09:44:14.318448Z",
     "start_time": "2019-04-10T09:44:14.312431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.376e+01, 1.503e+01, 1.000e+01, 9.010e+00, 6.340e+00, 2.240e+00,\n",
       "       1.800e+00, 1.700e+00, 1.000e-01, 1.000e-02])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(100*eigVals,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T12:26:20.231603Z",
     "start_time": "2019-04-08T12:26:20.217569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53.7645, 15.0258,  9.997 ,  9.0107,  6.3449,  2.2406,  1.8031,\n",
       "        1.7031])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(100*model.explainedVariance.toArray(),4)\n",
    "# np.round(100.00*model.explainedVariance.toArray(),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:06:14.474621Z",
     "start_time": "2019-04-09T09:06:14.132731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is PwC_OS\n",
      " Volume Serial Number is 0AC3-9136\n",
      "\n",
      " Directory of C:\\Users\\aoladipo001\\Desktop\\Fraud Detection Machine\\Machine Learning\\structure\n",
      "\n",
      "08/04/2019  13:36    <DIR>          .\n",
      "08/04/2019  13:36    <DIR>          ..\n",
      "01/04/2019  16:42               315 .gitignore\n",
      "01/04/2019  16:25    <DIR>          .ipynb_checkpoints\n",
      "01/04/2019  16:27            12,007 Customer_data1.csv\n",
      "08/04/2019  13:36            81,556 Fraud Detection system-spark model-final (with PCA).ipynb\n",
      "26/03/2019  11:48         6,661,191 New_Aggregated_data_final.csv\n",
      "               4 File(s)      6,755,069 bytes\n",
      "               3 Dir(s)  42,425,581,568 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T11:04:44.441097Z",
     "start_time": "2019-04-08T11:04:44.436086Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.085998Z",
     "start_time": "2019-03-26T13:54:59.050906Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = train.withColumn(\"Birthdate\",from_unixtime(unix_timestamp(train['dob'], 'MM/dd/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.336656Z",
     "start_time": "2019-03-26T13:54:59.301564Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train=(train.withColumn('Yearofbirth',year(train['Birthdate'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.529162Z",
     "start_time": "2019-03-26T13:54:59.494070Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = train.withColumn(\"trans_date\",from_unixtime(unix_timestamp(train['trans_date'], 'MM/dd/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.983356Z",
     "start_time": "2019-03-26T13:54:59.922195Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train=(train.withColumn('Month',month(train['fulltime'])))\n",
    "train = train.withColumn(\"Time\",hour(train[\"fulltime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:55:00.724302Z",
     "start_time": "2019-03-26T13:55:00.254066Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---------+\n",
      "|today_date|Age|      dob|\n",
      "+----------+---+---------+\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "+----------+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = train.withColumn('today_date',lit(2019))\n",
    "train = train.withColumn('Age',train['today_date']-train['Yearofbirth'])\n",
    "train.select('today_date','Age','dob').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:55:51.517678Z",
     "start_time": "2019-03-26T13:55:48.158873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"false\")\n",
    "a=train.drop('first','last')\n",
    "train2 = a.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:56:58.845504Z",
     "start_time": "2019-03-26T13:56:58.534689Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2[\"fulltimepd\"] =  pd.to_datetime(train2['unix_time'],unit='s')\n",
    "\n",
    "train2[\"Weekday\"] = train2[\"fulltimepd\"].dt.strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:00.004543Z",
     "start_time": "2019-03-26T13:56:59.972459Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#train[\"Time_short_for_grouping\"] = train[\"Time_short_for_grouping\"].astype(int)\n",
    "bins = [2,6,11,18,22]\n",
    "labels = [\"Early Morning\",\"Morning\",\"Afternoon\",\"Evening\"]\n",
    "train2[\"Time of day\"] = pd.cut(train2.Time,bins=bins,labels=labels)\n",
    "train2[\"Time of day\"]=train2[\"Time of day\"].cat.add_categories('Midnight') \n",
    "train2[\"Time of day\"] = train2[\"Time of day\"].fillna('Midnight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:01.047277Z",
     "start_time": "2019-03-26T13:57:00.713401Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2[\"Month\"] = train2[\"fulltimepd\"].dt.strftime(\"%B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:01.254821Z",
     "start_time": "2019-03-26T13:57:01.244795Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def haversine_(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate haversine distance between two co-ordinates\"\"\"\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:09.204661Z",
     "start_time": "2019-03-26T13:57:09.198646Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def manhattan_distance_pd(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate manhatten distance between pick_drop\"\"\"\n",
    "    a = haversine_(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_(lat1, lng1, lat2, lng1)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:13.340502Z",
     "start_time": "2019-03-26T13:57:09.618747Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2['hvsine']= haversine_(train2['lat'].values,\n",
    "                                 train2['long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:53.564944Z",
     "start_time": "2019-03-26T14:14:53.551908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2['mnhtn']= manhattan_distance_pd(train2['lat'].values,\n",
    "                                 train2['long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:54.052223Z",
     "start_time": "2019-03-26T14:14:54.039189Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in arcsin\n"
     ]
    }
   ],
   "source": [
    "train2['hvsine2']= haversine_(train2['prev_lat'].values,\n",
    "                                 train2['prev_long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:54.583620Z",
     "start_time": "2019-03-26T14:14:54.570585Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in arcsin\n"
     ]
    }
   ],
   "source": [
    "train2['mnhtn2']= manhattan_distance_pd(train2['prev_lat'].values,\n",
    "                                 train2['prev_long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:55.356650Z",
     "start_time": "2019-03-26T14:14:55.059869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Distance and time dfference\n",
    "train2['distandtime1'] = train2['mnhtn2']/train2['time_diff_min']\n",
    "# train2['distandtime2'] = train2['hvsine2']/train2['time_diff_min']\n",
    "# train['distandtime3'] = train['bearing2']/train['time_diff_min']\n",
    "\n",
    "#train2['distandtime'] =train2['manhtn']/train2['time_diff_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:55.529104Z",
     "start_time": "2019-03-26T14:14:55.519080Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Amount versus year average\n",
    "train2['amt_yrAvg'] = train2['amt']/train2['yrAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:56.213903Z",
     "start_time": "2019-03-26T14:14:56.123665Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "train3=train2.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:57.139335Z",
     "start_time": "2019-03-26T14:14:57.124295Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainimp_f = train3[['Channel', 'Transaction Type', 'gender', 'amt', 'Balance',\n",
    "       'Month', 'Weekday','Time of day','Age','mnhtn','sum_prev_day', 'cnt_prev_day_onl', 'sum_prev_day_onl',\n",
    "       '24hrsAvg','qtrAvg','wkAvg', 'monAvg','yrAvg','mnhtn2','distandtime1','amt_yrAvg','is_fraud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:57.773000Z",
     "start_time": "2019-03-26T14:14:57.724876Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel                 0\n",
       "Transaction Type        0\n",
       "gender                  0\n",
       "amt                     0\n",
       "Balance                 0\n",
       "Month                   0\n",
       "Weekday                 0\n",
       "Time of day             0\n",
       "Age                     0\n",
       "mnhtn                   0\n",
       "sum_prev_day         1496\n",
       "cnt_prev_day_onl    15761\n",
       "sum_prev_day_onl    17759\n",
       "24hrsAvg             1496\n",
       "qtrAvg                332\n",
       "wkAvg                 332\n",
       "monAvg                332\n",
       "yrAvg                 332\n",
       "mnhtn2                 98\n",
       "distandtime1           98\n",
       "amt_yrAvg             332\n",
       "is_fraud                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimp_f.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:58.316428Z",
     "start_time": "2019-03-26T14:14:58.311414Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "colsna= ['sum_prev_day','cnt_prev_day_onl','sum_prev_day_onl',\n",
    "         '24hrsAvg','qtrAvg','wkAvg','monAvg','yrAvg','mnhtn2','distandtime1','amt_yrAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:59.240858Z",
     "start_time": "2019-03-26T14:14:58.849830Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "for col in colsna:\n",
    "    trainimp_f[col] = trainimp_f[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:15:50.759324Z",
     "start_time": "2019-03-26T14:15:50.705180Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Transaction Type</th>\n",
       "      <th>gender</th>\n",
       "      <th>amt</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Time of day</th>\n",
       "      <th>Age</th>\n",
       "      <th>mnhtn</th>\n",
       "      <th>...</th>\n",
       "      <th>sum_prev_day_onl</th>\n",
       "      <th>24hrsAvg</th>\n",
       "      <th>qtrAvg</th>\n",
       "      <th>wkAvg</th>\n",
       "      <th>monAvg</th>\n",
       "      <th>yrAvg</th>\n",
       "      <th>mnhtn2</th>\n",
       "      <th>distandtime1</th>\n",
       "      <th>amt_yrAvg</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>F</td>\n",
       "      <td>95</td>\n",
       "      <td>942</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Midnight</td>\n",
       "      <td>45</td>\n",
       "      <td>1.688628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>F</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>45</td>\n",
       "      <td>2.709298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.020535</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Web</td>\n",
       "      <td>Online</td>\n",
       "      <td>F</td>\n",
       "      <td>188</td>\n",
       "      <td>6746</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>45</td>\n",
       "      <td>1.880668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.175526</td>\n",
       "      <td>0.022514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mobile</td>\n",
       "      <td>USSD</td>\n",
       "      <td>F</td>\n",
       "      <td>100</td>\n",
       "      <td>133</td>\n",
       "      <td>January</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>45</td>\n",
       "      <td>1.911466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.0</td>\n",
       "      <td>3.791882</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATM</td>\n",
       "      <td>ATM</td>\n",
       "      <td>F</td>\n",
       "      <td>79</td>\n",
       "      <td>3115</td>\n",
       "      <td>January</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>45</td>\n",
       "      <td>1.804176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.529470</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.669492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Channel Transaction Type gender  amt  Balance    Month   Weekday  \\\n",
       "0     POS              POS      F   95      942  January    Friday   \n",
       "1     POS              POS      F   90        0  January    Friday   \n",
       "2     Web           Online      F  188     6746  January    Friday   \n",
       "3  Mobile             USSD      F  100      133  January  Saturday   \n",
       "4     ATM              ATM      F   79     3115  January    Sunday   \n",
       "\n",
       "  Time of day  Age     mnhtn    ...     sum_prev_day_onl  24hrsAvg  qtrAvg  \\\n",
       "0    Midnight   45  1.688628    ...                  0.0      0.00    0.00   \n",
       "1     Evening   45  2.709298    ...                  0.0      0.00    0.00   \n",
       "2     Evening   45  1.880668    ...                  0.0      0.00    0.00   \n",
       "3   Afternoon   45  1.911466    ...                  0.0    124.33  124.33   \n",
       "4     Morning   45  1.804176    ...                  0.0    100.00  118.25   \n",
       "\n",
       "    wkAvg  monAvg  yrAvg    mnhtn2  distandtime1  amt_yrAvg  is_fraud  \n",
       "0    0.00    0.00    0.0  0.000000      0.000000   0.000000         0  \n",
       "1    0.00    0.00    0.0  1.020535      0.001001   0.000000         0  \n",
       "2    0.00    0.00    0.0  2.175526      0.022514   0.000000         0  \n",
       "3  124.33  124.33  124.0  3.791882      0.003087   0.806452         0  \n",
       "4  118.25  118.25  118.0  1.529470      0.001447   0.669492         0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimp_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:16:53.811654Z",
     "start_time": "2019-03-26T14:16:53.807642Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, StringType,\n",
    "                              IntegerType, StructType, DoubleType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:16:54.662895Z",
     "start_time": "2019-03-26T14:16:54.626801Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_schema = StructType([StructField(\"Channel\", StringType(), True),\n",
    "               StructField(\"Transaction Type\", StringType(), True),\n",
    "               StructField(\"gender\", StringType(), True),\n",
    "               StructField(\"amt\", IntegerType(), True),\n",
    "               StructField(\"Balance\", IntegerType(), True),\n",
    "               StructField(\"Month\", StringType(), True),\n",
    "               StructField(\"Weekday\", StringType(), True),\n",
    "               StructField(\"Time of day\", StringType(), True),\n",
    "               StructField(\"Age\", IntegerType(), True),\n",
    "               StructField(\"mnhtn\",  DoubleType(), True),\n",
    "               StructField(\"sum_prev_day\", DoubleType(), True),\n",
    "               StructField(\"sum_prev_day_onl\", DoubleType(), True),\n",
    "               StructField(\"cnt_prev_day_onl\", DoubleType(), True),\n",
    "               StructField(\"24hrsAvg\", DoubleType(), True),\n",
    "               StructField(\"wkAvg\", DoubleType(), True),\n",
    "               StructField(\"monAvg\", DoubleType(), True),\n",
    "               StructField(\"qtrAvg\", DoubleType(), True),\n",
    "               StructField(\"yrAvg\", DoubleType(), True),\n",
    "               StructField(\"mnhtn2\", DoubleType(), True),\n",
    "               StructField(\"distandtime1\", DoubleType(), True),\n",
    "               StructField(\"amt_yrAvg\", DoubleType(), True),\n",
    "               StructField(\"is_fraud\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:21:38.599671Z",
     "start_time": "2019-03-26T14:21:37.412548Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Convert\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"false\")\n",
    "#from pyspark.sql.types import *\n",
    "#from pyspark.sql import SqlContext\n",
    "trainimp_f = spark.createDataFrame(trainimp_f,schema=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:21:44.399907Z",
     "start_time": "2019-03-26T14:21:43.251890Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MLlib Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:51:23.301547Z",
     "start_time": "2019-03-26T14:50:43.764116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import (RandomForestClassifier,\n",
    "                                       GBTClassifier)\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, \n",
    "                          outputCol=column+\"_index\",\n",
    "                          handleInvalid=\"keep\").fit(trainimp_f) \n",
    "            for column in list(['Transaction Type',\n",
    "                         'gender',\n",
    "                        \"Time of day\"]) ]\n",
    "\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols= ['amt',\n",
    "                                        \"Age\",\n",
    "                                        \"Time of day_index\",\n",
    "                                        'amt_yrAvg','mnhtn2','distandtime1'], \n",
    "                            outputCol='features')\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(labelCol='is_fraud',\n",
    "                             featuresCol= 'features',\n",
    "                             maxDepth = 11,numTrees=40,seed=1)    \n",
    "\n",
    "pipeline = Pipeline(stages=indexers+[assembler,rfc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:06:05.343015Z",
     "start_time": "2019-03-26T15:06:05.260798Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data,test_data = trainimp_f.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:07:23.593164Z",
     "start_time": "2019-03-26T15:06:06.636451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rfc_model2 = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:17:03.102435Z",
     "start_time": "2019-03-26T15:17:02.829710Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rfc_preds3 = rfc_model2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:17:36.370916Z",
     "start_time": "2019-03-26T15:17:32.962851Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+--------------------+--------+----------+\n",
      "|amt|             mnhtn2|        distandtime1|           amt_yrAvg|is_fraud|prediction|\n",
      "+---+-------------------+--------------------+--------------------+--------+----------+\n",
      "|  6| 0.6004767007199595|7.269931350297886E-5| 0.06976744186046512|       0|       0.0|\n",
      "|  6|  5.278624833901226| 0.04529064636551888|0.047619047619047616|       0|       0.0|\n",
      "|  6|0.21698341972920043|0.007215943456242116| 0.07407407407407407|       0|       0.0|\n",
      "|  6|  2.817629414586735|0.020340957367793348| 0.06451612903225806|       0|       0.0|\n",
      "|  7| 0.6871846682760476|0.002072017694183771| 0.07865168539325842|       0|       0.0|\n",
      "|  7| 1.3819106657909628|0.002747992892521...| 0.07777777777777778|       0|       0.0|\n",
      "|  7|   3.03338696845326|0.003177818834480394| 0.12727272727272726|       0|       0.0|\n",
      "|  8|   2.28200209680964|0.018108253426516747| 0.07920792079207921|       0|       0.0|\n",
      "|  8|  1.996620501516868|0.024142932303710618| 0.08421052631578947|       0|       0.0|\n",
      "|  8|  2.927938207222214|0.013667265122635552| 0.03755868544600939|       0|       0.0|\n",
      "|  9|  3.776701133946403| 0.12555522386789905|               0.125|       0|       0.0|\n",
      "|  9| 1.4197449184394146| 0.17271835990747136|  0.0891089108910891|       0|       0.0|\n",
      "|  9| 0.5586446235335906|1.358769819364670...| 0.09183673469387756|       0|       0.0|\n",
      "|  9| 0.6513777438956946|8.185708374435371E-4| 0.09574468085106383|       0|       0.0|\n",
      "| 10|  1.607151293804825|0.002260505075889...| 0.11904761904761904|       0|       0.0|\n",
      "| 10| 1.8371085927863857|0.001185292526573...| 0.10638297872340426|       0|       0.0|\n",
      "| 11| 2.0521475670796705|0.001597648516971592| 0.21153846153846154|       0|       0.0|\n",
      "| 11|  2.992614292679944| 0.07485278370885304| 0.14864864864864866|       0|       0.0|\n",
      "| 11|   6.62659210532812| 0.03657059660777109| 0.04044117647058824|       0|       0.0|\n",
      "| 12|  2.501658977353707|0.023041899026929234| 0.14457831325301204|       0|       0.0|\n",
      "+---+-------------------+--------------------+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_preds3.select('amt','mnhtn2','distandtime1','amt_yrAvg','is_fraud','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995282126010989\n"
     ]
    }
   ],
   "source": [
    "my_binary_eval = BinaryClassificationEvaluator(labelCol='is_fraud')\n",
    "print(my_binary_eval.evaluate(rfc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 170\n",
      "True Negatives: 6058\n",
      "False Positives: 19\n",
      "False Negatives: 3\n",
      "Total 6250\n"
     ]
    }
   ],
   "source": [
    "tp = rfc_preds[(rfc_preds.is_fraud == 1) & (rfc_preds.prediction == 1)].count()\n",
    "tn = rfc_preds[(rfc_preds.is_fraud == 0) & (rfc_preds.prediction == 0)].count()\n",
    "fp = rfc_preds[(rfc_preds.is_fraud == 0) & (rfc_preds.prediction == 1)].count()\n",
    "fn = rfc_preds[(rfc_preds.is_fraud == 1) & (rfc_preds.prediction == 0)].count()\n",
    "print (\"True Positives:\", tp)\n",
    "print (\"True Negatives:\", tn)\n",
    "print (\"False Positives:\", fp)\n",
    "print (\"False Negatives:\", fn)\n",
    "print (\"Total\", rfc_preds.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 156\n",
      "True Negatives: 6065\n",
      "False Positives: 5\n",
      "False Negatives: 19\n",
      "Total 6245\n"
     ]
    }
   ],
   "source": [
    "tp = gbt_preds[(gbt_preds.is_fraud == 1) & (gbt_preds.prediction == 1)].count()\n",
    "tn = gbt_preds[(gbt_preds.is_fraud == 0) & (gbt_preds.prediction == 0)].count()\n",
    "fp = gbt_preds[(gbt_preds.is_fraud == 0) & (gbt_preds.prediction == 1)].count()\n",
    "fn = gbt_preds[(gbt_preds.is_fraud == 1) & (gbt_preds.prediction == 0)].count()\n",
    "print (\"True Positives:\", tp)\n",
    "print (\"True Negatives:\", tn)\n",
    "print (\"False Positives:\", fp)\n",
    "print (\"False Negatives:\", fn)\n",
    "print (\"Total\", gbt_preds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Saving & Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rfc_model.write().overwrite().save(\"models/rfc_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.RandomForestClassificationModel"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rfc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed string column 'category' to indexed column 'categoryIndex'\n",
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "StringIndexer will store labels in output column metadata\n",
      "\n",
      "Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n",
      "+---+-------------+----------------+\n",
      "| id|categoryIndex|originalCategory|\n",
      "+---+-------------+----------------+\n",
      "|  0|          0.0|               a|\n",
      "|  1|          2.0|               b|\n",
      "|  2|          1.0|               c|\n",
      "|  3|          0.0|               a|\n",
      "|  4|          0.0|               a|\n",
      "|  5|          1.0|               c|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "print(\"Transformed string column '%s' to indexed column '%s'\"\n",
    "      % (indexer.getInputCol(), indexer.getOutputCol()))\n",
    "indexed.show()\n",
    "\n",
    "print(\"StringIndexer will store labels in output column metadata\\n\")\n",
    "\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
    "converted = converter.transform(indexed)\n",
    "\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
    "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
    "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
