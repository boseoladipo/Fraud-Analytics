{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T11:27:51.237675Z",
     "start_time": "2019-04-10T11:27:51.172502Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T11:28:03.945181Z",
     "start_time": "2019-04-10T11:27:51.244695Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T11:28:17.300730Z",
     "start_time": "2019-04-10T11:28:03.956206Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T11:30:29.436828Z",
     "start_time": "2019-04-10T11:30:29.430815Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def estimateCovariance(df):\n",
    "    \"\"\"Compute the covariance matrix for a given dataframe.\n",
    "\n",
    "    Note:\n",
    "        The multi-dimensional covariance array should be calculated using outer products.  Don't\n",
    "        forget to normalize the data by first subtracting the mean.\n",
    "\n",
    "    Args:\n",
    "        df:  A Spark dataframe with a column named 'features', which (column) consists of DenseVectors.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the\n",
    "            length of the arrays in the input dataframe.\n",
    "    \"\"\"\n",
    "    m = df.select(df['features']).rdd.map(lambda x: x[0]).mean()\n",
    "    dfZeroMean = df.select(df['features']).rdd.map(lambda x:   x[0]).map(lambda x: x-m)  # subtract the mean\n",
    "\n",
    "    return dfZeroMean.map(lambda x: np.outer(x,x)).sum()/df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T12:00:09.279920Z",
     "start_time": "2019-04-10T12:00:09.264882Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import eigh\n",
    "\n",
    "def pca(df, k=2):\n",
    "    \"\"\"Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n",
    "\n",
    "    Note:\n",
    "        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
    "        each eigenvectors as a column.  This function should also return eigenvectors as columns.\n",
    "\n",
    "    Args:\n",
    "        df: A Spark dataframe with a 'features' column, which (column) consists of DenseVectors.\n",
    "        k (int): The number of principal components to return.\n",
    "\n",
    "    Returns:\n",
    "        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of\n",
    "        scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of\n",
    "        rows equals the length of the arrays in the input `RDD` and the number of columns equals\n",
    "        `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays\n",
    "        of length `k`.  Eigenvalues is an array of length d (the number of features).\n",
    "     \"\"\"\n",
    "    cov = estimateCovariance(df)\n",
    "    col = cov.shape[1]\n",
    "    eigVals, eigVecs = eigh(cov)\n",
    "    inds = np.argsort(eigVals)\n",
    "    eigVecs = eigVecs.T[inds[-1:-(col+1):-1]]  \n",
    "    components = eigVecs[0:k]\n",
    "    eigVals = eigVals[inds[-1:-(col+1):-1]]  # sort eigenvals\n",
    "    score = df.select(df['features']).rdd.map(lambda x: x[0]).map(lambda x: np.dot(x, components.T) )\n",
    "    # Return the `k` principal components, `k` scores, and all eigenvalues\n",
    "\n",
    "    return components.T, score, eigVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T12:01:09.853671Z",
     "start_time": "2019-04-10T12:00:09.551329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.0, 1.0, 0.0, 7.0, 0.0]), pca_features=DenseVector([1.6486, -4.0133])),\n",
       " Row(features=DenseVector([2.0, 0.0, 3.0, 4.0, 5.0]), pca_features=DenseVector([-4.6451, -1.1168])),\n",
       " Row(features=DenseVector([4.0, 0.0, 0.0, 6.0, 7.0]), pca_features=DenseVector([-6.4289, -5.338]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(Vectors.dense([0.0, 1.0, 0.0, 7.0, 0.0]),),\n",
    "     (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "     (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data,[\"features\"])\n",
    "pca_extracted = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model = pca_extracted.fit(df)\n",
    "model.transform(df).collect()\n",
    "\n",
    "# [Row(features=DenseVector([0.0, 1.0, 0.0, 7.0, 0.0]), pca_features=DenseVector([1.6486, -4.0133])),\n",
    "# Row(features=DenseVector([2.0, 0.0, 3.0, 4.0, 5.0]), pca_features=DenseVector([-4.6451, -1.1168])),\n",
    "# Row(features=DenseVector([4.0, 0.0, 0.0, 6.0, 7.0]), pca_features=DenseVector([-6.4289, -5.338]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T12:01:54.738494Z",
     "start_time": "2019-04-10T12:01:09.856680Z"
    }
   },
   "outputs": [],
   "source": [
    "comp, score, eigVals = pca(df)\n",
    "#  score.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T11:36:20.107008Z",
     "start_time": "2019-04-10T11:35:32.498Z"
    }
   },
   "outputs": [],
   "source": [
    "eigVals.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
