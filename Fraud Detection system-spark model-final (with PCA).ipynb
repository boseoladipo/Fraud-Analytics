{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:41:46.960723Z",
     "start_time": "2019-04-11T10:41:46.922622Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:41:57.959625Z",
     "start_time": "2019-04-11T10:41:46.965738Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:42:14.952276Z",
     "start_time": "2019-04-11T10:41:57.964638Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:42:29.486466Z",
     "start_time": "2019-04-11T10:42:14.957290Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data = spark.read.csv(\"New_Aggregated_data_final.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:42:30.212374Z",
     "start_time": "2019-04-11T10:42:29.491480Z"
    }
   },
   "outputs": [],
   "source": [
    "customer_data = spark.read.csv(\"Customer_data1.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:42:30.740762Z",
     "start_time": "2019-04-11T10:42:30.216385Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data = trans_data.withColumn(\"sum_prev_day_onl\", trans_data[\"sum_prev_day_onl\"].cast(\"integer\"))\n",
    "trans_data = trans_data.withColumn(\"sum_prev_day_mon_onl\", trans_data[\"sum_prev_day_mon_onl\"].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:42:31.040550Z",
     "start_time": "2019-04-11T10:42:30.744774Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data1 = trans_data.select('amt', 'Balance',\n",
    "      'sum_prev_day', 'cnt_prev_day_onl', 'sum_prev_day_onl',\n",
    "       '24hrsAvg','qtrAvg','wkAvg', 'monAvg','yrAvg').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:51:06.841188Z",
     "start_time": "2019-04-11T10:51:06.746939Z"
    }
   },
   "outputs": [],
   "source": [
    "# merging the data together by their unique \"id\"\n",
    "train = trans_data.join(customer_data,how='left',on='cc_num')\n",
    "# all_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:42:31.050576Z",
     "start_time": "2019-04-11T10:42:31.044562Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# train.where(col('cc_num').isNull()).count()\n",
    "# df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "# customer_data.select([count(when(col(c).isNull(),c)).alias(c) for c in customer_data.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:42:31.099704Z",
     "start_time": "2019-04-11T10:42:31.054587Z"
    },
    "code_folding": [
     0,
     17
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorizedf(inputdf,vars='all'):\n",
    "        \"\"\"Returns the input spark dataframe with an additional column of dense vector features\"\"\"\n",
    "        if vars=='all':\n",
    "            myfeatures = inputdf.columns\n",
    "        else:\n",
    "            myfeatures=vars\n",
    "        \n",
    "        # assemble features\n",
    "        assembler = VectorAssembler(inputCols = myfeatures,outputCol=\"features\")\n",
    "        assembled = assembler.transform(inputdf).select('features')\n",
    "        \n",
    "        # scale and standardize data\n",
    "        scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
    "        scaled = scaler.fit(assembled).transform(assembled)\n",
    "\n",
    "        return myfeatures, scaled\n",
    "\n",
    "def estimateCovariance(df):\n",
    "    \"\"\"Compute the covariance matrix for a given dataframe.\n",
    "\n",
    "    Note:\n",
    "        The multi-dimensional covariance array should be calculated using outer products.  Don't\n",
    "        forget to normalize the data by first subtracting the mean.\n",
    "\n",
    "    Args:\n",
    "        df:  A Spark dataframe with a column named 'scaledFeatures', which (column) consists of DenseVectors.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the\n",
    "            length of the arrays in the input dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    dfZeroMean = df.select('scaledFeatures')\n",
    "\n",
    "    return dfZeroMean.rdd.map(lambda x: np.outer(x,x)).sum()/df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:42:31.123769Z",
     "start_time": "2019-04-11T10:42:31.103717Z"
    },
    "code_folding": [
     2
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import eigh\n",
    "\n",
    "def computeEigVals(df):\n",
    "    \"\"\"Computes all eigenvalues.\n",
    "\n",
    "    Note:\n",
    "        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
    "        each eigenvectors as a column. \n",
    "\n",
    "    Args:\n",
    "        df: A Spark dataframe with a 'features' column, which (column) consists of DenseVectors.\n",
    "\n",
    "    Returns:\n",
    "        Eigenvalues is an array of length d (the number of features).\n",
    "     \"\"\"\n",
    "    dfT = vectorizedf(df)[1]\n",
    "    cov = estimateCovariance(dfT)\n",
    "    col = cov.shape[1]\n",
    "    eigVals, eigVecs = eigh(cov)\n",
    "    inds = np.argsort(eigVals)\n",
    "    eigVecs = eigVecs.T[inds[-1:-(col+1):-1]]  \n",
    "    eigVals = eigVals[inds[-1:-(col+1):-1]]  # sort eigenvals\n",
    "\n",
    "    return eigVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:42:31.152845Z",
     "start_time": "2019-04-11T10:42:31.128783Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def computePCA(df):\n",
    "    \"\"\"\n",
    "    Compute Principal components using pyspark.ml inbuilt function\n",
    "    \n",
    "    Args: \n",
    "        df: A Spark dataframe\n",
    "        k: The number of principal components to be computed which must be less than or equal\n",
    "            to the number of variables in the dataframe\n",
    "            \n",
    "    Returns:\n",
    "        Spark dataframe with contributions of each variable to each principal component. Columns\n",
    "        are principal components, rows are variable names\n",
    "    \n",
    "    \"\"\"\n",
    "    # compute PCA\n",
    "    pca = PCA(k=10, inputCol='scaledFeatures', outputCol='pcaFeatures')\n",
    "    cols, dfT = vectorizedf(df)\n",
    "    model = pca.fit(dfT)\n",
    "    transformed_feature = model.transform(dfT)\n",
    "    \n",
    "    # compute number of significant principal components\n",
    "    totalpercent = np.cumsum(model.explainedVariance.toArray())\n",
    "    numsigpcs = np.where(totalpercent > 0.85)[0][0]\n",
    "    \n",
    "    # create dataframe of mapping\n",
    "    pcs = np.round(model.pc.toArray(),4)[:,:numsigpcs]\n",
    "#     df_pc = pd.DataFrame(pcs, columns = ['PC'+str(i) for i in range(1, len(cols)+1)], index = cols)\n",
    "    \n",
    "    return cols, numsigpcs, pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:42:31.185932Z",
     "start_time": "2019-04-11T10:42:31.158861Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extractImpVars(n, pcs, eigVals, cols):\n",
    "    \"\"\"\n",
    "    Order the results from the PCA computation to find the variables with the greatest contributions\n",
    "    \n",
    "    Args:\n",
    "        n: number of significant principal components from pca computation (85%)\n",
    "        pcs: computed principal components\n",
    "        eigVals: computed eigenvalues of the principal components\n",
    "        cols: list of columns used to compute  principal components\n",
    "        \n",
    "    Returns:\n",
    "        SigVars: a list of the most significant variables\n",
    "    \"\"\"\n",
    "    contrib = (pcs[:,:n]*eigVals[:n]).sum(axis=1)\n",
    "    \n",
    "    contribdf = pd.DataFrame(contrib, columns = ['PC'], index = cols)\n",
    "    \n",
    "    impVars = contribdf.sort_values(by=['PC']).index[:4].tolist()\n",
    "    \n",
    "    return impVars    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:42:31.225034Z",
     "start_time": "2019-04-11T10:42:31.191948Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pcaColumnSelector(df):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with top columns selected by performing PCA\n",
    "    \"\"\"\n",
    "    eigVals = computeEigVals(trans_data1)\n",
    "    \n",
    "    cols, n, pcs = computePCA(trans_data1)\n",
    "    \n",
    "    impVars = extractImpVars(n, pcs, eigVals, cols)\n",
    "    \n",
    "    return df.select(impVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:43:01.257953Z",
     "start_time": "2019-04-11T10:42:31.231053Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+\n",
      "|yrAvg|qtrAvg|monAvg| wkAvg|\n",
      "+-----+------+------+------+\n",
      "|  0.0|   0.0|   0.0|   0.0|\n",
      "|  0.0|   0.0|   0.0|   0.0|\n",
      "|  0.0|   0.0|   0.0|   0.0|\n",
      "|124.0|124.33|124.33|124.33|\n",
      "|118.0|118.25|118.25|118.25|\n",
      "|118.0|118.25|118.25|118.25|\n",
      "|116.0|116.33|116.33|116.33|\n",
      "|110.0|110.43|110.43|110.43|\n",
      "|105.0| 105.0| 105.0| 105.0|\n",
      "|105.0|105.33|105.33| 95.83|\n",
      "|105.0|105.33|105.33| 95.83|\n",
      "|102.0|101.64|101.64| 92.67|\n",
      "|102.0|101.64|101.64| 92.67|\n",
      "|100.0|100.38|100.38| 89.25|\n",
      "| 95.0| 95.14| 95.14| 71.33|\n",
      "| 92.0| 92.13| 92.13|  38.5|\n",
      "| 92.0| 92.13| 92.13|  38.5|\n",
      "| 98.0| 98.24| 98.24|112.67|\n",
      "| 98.0| 97.83| 97.83|107.25|\n",
      "|101.0|101.05|101.05| 117.6|\n",
      "+-----+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcaColumnSelector(trans_data1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:56:57.347280Z",
     "start_time": "2019-04-11T10:56:57.342266Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:56:57.392400Z",
     "start_time": "2019-04-11T10:56:57.351290Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = train.withColumn(\"Birthdate\",from_unixtime(unix_timestamp(train['dob'], 'MM/dd/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:56:57.472608Z",
     "start_time": "2019-04-11T10:56:57.395405Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train=(train.withColumn('Yearofbirth',year(train['Birthdate'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:56:57.525746Z",
     "start_time": "2019-04-11T10:56:57.476619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = train.withColumn(\"trans_date\",from_unixtime(unix_timestamp(train['trans_date'], 'MM/dd/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:56:57.602949Z",
     "start_time": "2019-04-11T10:56:57.529757Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train=(train.withColumn('Month',month(train['fulltime'])))\n",
    "train = train.withColumn(\"Time\",hour(train[\"fulltime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:56:58.608587Z",
     "start_time": "2019-04-11T10:56:57.606960Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---------+\n",
      "|today_date|Age|      dob|\n",
      "+----------+---+---------+\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "+----------+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = train.withColumn('today_date',lit(2019))\n",
    "train = train.withColumn('Age',train['today_date']-train['Yearofbirth'])\n",
    "train.select('today_date','Age','dob').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:16.005191Z",
     "start_time": "2019-04-11T10:56:58.612597Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"false\")\n",
    "a=train.drop('first','last')\n",
    "train2 = a.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:16.507507Z",
     "start_time": "2019-04-11T10:57:16.009203Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2[\"fulltimepd\"] =  pd.to_datetime(train2['unix_time'],unit='s')\n",
    "\n",
    "train2[\"Weekday\"] = train2[\"fulltimepd\"].dt.strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:16.535581Z",
     "start_time": "2019-04-11T10:57:16.511517Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#train[\"Time_short_for_grouping\"] = train[\"Time_short_for_grouping\"].astype(int)\n",
    "bins = [2,6,11,18,22]\n",
    "labels = [\"Early Morning\",\"Morning\",\"Afternoon\",\"Evening\"]\n",
    "train2[\"Time of day\"] = pd.cut(train2.Time,bins=bins,labels=labels)\n",
    "train2[\"Time of day\"]=train2[\"Time of day\"].cat.add_categories('Midnight') \n",
    "train2[\"Time of day\"] = train2[\"Time of day\"].fillna('Midnight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:16.910566Z",
     "start_time": "2019-04-11T10:57:16.540596Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2[\"Month\"] = train2[\"fulltimepd\"].dt.strftime(\"%B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:16.926607Z",
     "start_time": "2019-04-11T10:57:16.914577Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def haversine_(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate haversine distance between two co-ordinates\"\"\"\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:16.960696Z",
     "start_time": "2019-04-11T10:57:16.931621Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def manhattan_distance_pd(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate manhatten distance between pick_drop\"\"\"\n",
    "    a = haversine_(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_(lat1, lng1, lat2, lng1)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:17.516152Z",
     "start_time": "2019-04-11T10:57:16.965709Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2['hvsine']= haversine_(train2['lat'].values,\n",
    "                                 train2['long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:17.532194Z",
     "start_time": "2019-04-11T10:57:17.521166Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2['mnhtn']= manhattan_distance_pd(train2['lat'].values,\n",
    "                                 train2['long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:17.574306Z",
     "start_time": "2019-04-11T10:57:17.539214Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in arcsin\n"
     ]
    }
   ],
   "source": [
    "train2['hvsine2']= haversine_(train2['prev_lat'].values,\n",
    "                                 train2['prev_long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:17.610401Z",
     "start_time": "2019-04-11T10:57:17.580322Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in arcsin\n"
     ]
    }
   ],
   "source": [
    "train2['mnhtn2']= manhattan_distance_pd(train2['prev_lat'].values,\n",
    "                                 train2['prev_long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:17.993403Z",
     "start_time": "2019-04-11T10:57:17.616416Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Distance and time dfference\n",
    "train2['distandtime1'] = train2['mnhtn2']/train2['time_diff_min']\n",
    "# train2['distandtime2'] = train2['hvsine2']/train2['time_diff_min']\n",
    "# train['distandtime3'] = train['bearing2']/train['time_diff_min']\n",
    "\n",
    "#train2['distandtime'] =train2['manhtn']/train2['time_diff_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:18.008442Z",
     "start_time": "2019-04-11T10:57:17.996411Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Amount versus year average\n",
    "train2['amt_yrAvg'] = train2['amt']/train2['yrAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:18.115723Z",
     "start_time": "2019-04-11T10:57:18.036516Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "train3=train2.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:18.135776Z",
     "start_time": "2019-04-11T10:57:18.120737Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainimp_f = train3[['Channel', 'Transaction Type', 'gender', 'amt', 'Balance',\n",
    "       'Month', 'Weekday','Time of day','Age','mnhtn','sum_prev_day', 'cnt_prev_day_onl', 'sum_prev_day_onl',\n",
    "       '24hrsAvg','qtrAvg','wkAvg', 'monAvg','yrAvg','mnhtn2','distandtime1','amt_yrAvg','is_fraud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:18.221001Z",
     "start_time": "2019-04-11T10:57:18.141793Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel                 0\n",
       "Transaction Type        0\n",
       "gender                  0\n",
       "amt                     0\n",
       "Balance                 0\n",
       "Month                   0\n",
       "Weekday                 0\n",
       "Time of day             0\n",
       "Age                     0\n",
       "mnhtn                   0\n",
       "sum_prev_day         1496\n",
       "cnt_prev_day_onl    15761\n",
       "sum_prev_day_onl    17759\n",
       "24hrsAvg             1496\n",
       "qtrAvg                332\n",
       "wkAvg                 332\n",
       "monAvg                332\n",
       "yrAvg                 332\n",
       "mnhtn2                 98\n",
       "distandtime1           98\n",
       "amt_yrAvg             332\n",
       "is_fraud                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimp_f.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:18.233033Z",
     "start_time": "2019-04-11T10:57:18.226015Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "colsna= ['sum_prev_day','cnt_prev_day_onl','sum_prev_day_onl',\n",
    "         '24hrsAvg','qtrAvg','wkAvg','monAvg','yrAvg','mnhtn2','distandtime1','amt_yrAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:18.747378Z",
     "start_time": "2019-04-11T10:57:18.251080Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "for col in colsna:\n",
    "    trainimp_f[col] = trainimp_f[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:19.025111Z",
     "start_time": "2019-04-11T10:57:18.753397Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Transaction Type</th>\n",
       "      <th>gender</th>\n",
       "      <th>amt</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Time of day</th>\n",
       "      <th>Age</th>\n",
       "      <th>mnhtn</th>\n",
       "      <th>...</th>\n",
       "      <th>sum_prev_day_onl</th>\n",
       "      <th>24hrsAvg</th>\n",
       "      <th>qtrAvg</th>\n",
       "      <th>wkAvg</th>\n",
       "      <th>monAvg</th>\n",
       "      <th>yrAvg</th>\n",
       "      <th>mnhtn2</th>\n",
       "      <th>distandtime1</th>\n",
       "      <th>amt_yrAvg</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>F</td>\n",
       "      <td>95</td>\n",
       "      <td>942</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Midnight</td>\n",
       "      <td>45</td>\n",
       "      <td>1.688628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>F</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>45</td>\n",
       "      <td>2.709298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.020535</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Web</td>\n",
       "      <td>Online</td>\n",
       "      <td>F</td>\n",
       "      <td>188</td>\n",
       "      <td>6746</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>45</td>\n",
       "      <td>1.880668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.175526</td>\n",
       "      <td>0.022514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mobile</td>\n",
       "      <td>USSD</td>\n",
       "      <td>F</td>\n",
       "      <td>100</td>\n",
       "      <td>133</td>\n",
       "      <td>January</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>45</td>\n",
       "      <td>1.911466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.0</td>\n",
       "      <td>3.791882</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATM</td>\n",
       "      <td>ATM</td>\n",
       "      <td>F</td>\n",
       "      <td>79</td>\n",
       "      <td>3115</td>\n",
       "      <td>January</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>45</td>\n",
       "      <td>1.804176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.529470</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.669492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Channel Transaction Type gender  amt  Balance    Month   Weekday  \\\n",
       "0     POS              POS      F   95      942  January    Friday   \n",
       "1     POS              POS      F   90        0  January    Friday   \n",
       "2     Web           Online      F  188     6746  January    Friday   \n",
       "3  Mobile             USSD      F  100      133  January  Saturday   \n",
       "4     ATM              ATM      F   79     3115  January    Sunday   \n",
       "\n",
       "  Time of day  Age     mnhtn    ...     sum_prev_day_onl  24hrsAvg  qtrAvg  \\\n",
       "0    Midnight   45  1.688628    ...                  0.0      0.00    0.00   \n",
       "1     Evening   45  2.709298    ...                  0.0      0.00    0.00   \n",
       "2     Evening   45  1.880668    ...                  0.0      0.00    0.00   \n",
       "3   Afternoon   45  1.911466    ...                  0.0    124.33  124.33   \n",
       "4     Morning   45  1.804176    ...                  0.0    100.00  118.25   \n",
       "\n",
       "    wkAvg  monAvg  yrAvg    mnhtn2  distandtime1  amt_yrAvg  is_fraud  \n",
       "0    0.00    0.00    0.0  0.000000      0.000000   0.000000         0  \n",
       "1    0.00    0.00    0.0  1.020535      0.001001   0.000000         0  \n",
       "2    0.00    0.00    0.0  2.175526      0.022514   0.000000         0  \n",
       "3  124.33  124.33  124.0  3.791882      0.003087   0.806452         0  \n",
       "4  118.25  118.25  118.0  1.529470      0.001447   0.669492         0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimp_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:19.040152Z",
     "start_time": "2019-04-11T10:57:19.031125Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, StringType,\n",
    "                              IntegerType, StructType, DoubleType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:19.104320Z",
     "start_time": "2019-04-11T10:57:19.045162Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_schema = StructType([StructField(\"Channel\", StringType(), True),\n",
    "               StructField(\"Transaction Type\", StringType(), True),\n",
    "               StructField(\"gender\", StringType(), True),\n",
    "               StructField(\"amt\", IntegerType(), True),\n",
    "               StructField(\"Balance\", IntegerType(), True),\n",
    "               StructField(\"Month\", StringType(), True),\n",
    "               StructField(\"Weekday\", StringType(), True),\n",
    "               StructField(\"Time of day\", StringType(), True),\n",
    "               StructField(\"Age\", IntegerType(), True),\n",
    "               StructField(\"mnhtn\",  DoubleType(), True),\n",
    "               StructField(\"sum_prev_day\", DoubleType(), True),\n",
    "               StructField(\"sum_prev_day_onl\", DoubleType(), True),\n",
    "               StructField(\"cnt_prev_day_onl\", DoubleType(), True),\n",
    "               StructField(\"24hrsAvg\", DoubleType(), True),\n",
    "               StructField(\"wkAvg\", DoubleType(), True),\n",
    "               StructField(\"monAvg\", DoubleType(), True),\n",
    "               StructField(\"qtrAvg\", DoubleType(), True),\n",
    "               StructField(\"yrAvg\", DoubleType(), True),\n",
    "               StructField(\"mnhtn2\", DoubleType(), True),\n",
    "               StructField(\"distandtime1\", DoubleType(), True),\n",
    "               StructField(\"amt_yrAvg\", DoubleType(), True),\n",
    "               StructField(\"is_fraud\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:21.108569Z",
     "start_time": "2019-04-11T10:57:19.109329Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Convert\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"false\")\n",
    "#from pyspark.sql.types import *\n",
    "#from pyspark.sql import SqlContext\n",
    "trainimp_f = spark.createDataFrame(trainimp_f,schema=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:57:21.120601Z",
     "start_time": "2019-04-11T10:57:21.113583Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLlib Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:36:13.762296Z",
     "start_time": "2019-04-11T11:35:36.334704Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import (RandomForestClassifier,\n",
    "                                       GBTClassifier)\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, \n",
    "                          outputCol=column+\"_index\",\n",
    "                          handleInvalid=\"keep\").fit(trainimp_f) \n",
    "            for column in list(['Transaction Type',\n",
    "                         'gender',\n",
    "                        \"Time of day\"]) ]\n",
    "\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols= ['amt',\n",
    "                                        \"Age\",\n",
    "                                        \"Time of day_index\",\n",
    "                                        'amt_yrAvg','mnhtn2','distandtime1'], \n",
    "                            outputCol='features')\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(labelCol='is_fraud', predictionCol='rf_prediction', \n",
    "                             probabilityCol='rf_probability', rawPredictionCol='rf_rawPrediction',\n",
    "                             featuresCol= 'features',\n",
    "                             maxDepth = 11,numTrees=40,seed=1)    \n",
    "\n",
    "gbt = GBTClassifier(labelCol='is_fraud',featuresCol= 'features',\n",
    "                    predictionCol='gbt_prediction', maxIter=33, \n",
    "                    maxDepth = 3,stepSize=0.72,seed=1 )\n",
    "\n",
    "pipeline = Pipeline(stages=indexers+[assembler,rfc,gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:36:13.784355Z",
     "start_time": "2019-04-11T11:36:13.766307Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data,test_data = trainimp_f.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:38:05.615941Z",
     "start_time": "2019-04-11T11:36:13.788366Z"
    }
   },
   "outputs": [],
   "source": [
    "model2 = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:38:05.954835Z",
     "start_time": "2019-04-11T11:38:05.619953Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:43:36.275057Z",
     "start_time": "2019-04-11T11:43:32.910192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------------------------+------------------------------------------+--------------+\n",
      "|rf_prediction|rf_probability                            |probability                               |gbt_prediction|\n",
      "+-------------+------------------------------------------+------------------------------------------+--------------+\n",
      "|0.0          |[1.0,0.0]                                 |[0.9939985406750567,0.006001459324943337] |0.0           |\n",
      "|0.0          |[0.999731182795699,2.6881720430107527E-4] |[0.9940790517535676,0.0059209482464324115]|0.0           |\n",
      "|0.0          |[0.9996527777777778,3.472222222222222E-4] |[0.9921450526530743,0.007854947346925711] |0.0           |\n",
      "|0.0          |[1.0,0.0]                                 |[0.9942794426244382,0.005720557375561763] |0.0           |\n",
      "|0.0          |[1.0,0.0]                                 |[0.9939227878547261,0.0060772121452739025]|0.0           |\n",
      "|0.0          |[0.9982142857142857,0.0017857142857142857]|[0.9941412579297022,0.0058587420702977955]|0.0           |\n",
      "|0.0          |[0.9956247156329827,0.0043752843670172405]|[0.9930682944402823,0.006931705559717671] |0.0           |\n",
      "|0.0          |[0.999731182795699,2.6881720430107527E-4] |[0.9940790517535676,0.0059209482464324115]|0.0           |\n",
      "|0.0          |[0.9996576715173541,3.423284826459405E-4] |[0.9942794426244382,0.005720557375561763] |0.0           |\n",
      "|0.0          |[0.9922615986549012,0.007738401345098749] |[0.993141046481218,0.006858953518781963]  |0.0           |\n",
      "|0.0          |[0.9698124794944573,0.030187520505542704] |[0.9941412579297022,0.0058587420702977955]|0.0           |\n",
      "|0.0          |[0.9804023831294929,0.019597616870507102] |[0.9937370924603982,0.006262907539601792] |0.0           |\n",
      "|0.0          |[0.9894315386439901,0.010568461356009824] |[0.9800777294102199,0.019922270589780133] |0.0           |\n",
      "|0.0          |[0.9968190098945859,0.003180990105414027] |[0.9860572536847554,0.013942746315244614] |0.0           |\n",
      "|0.0          |[0.9805802877202616,0.01941971227973834]  |[0.9793901652384234,0.02060983476157663]  |0.0           |\n",
      "|0.0          |[0.9976341487199253,0.0023658512800746735]|[0.9862428246546094,0.01375717534539056]  |0.0           |\n",
      "|0.0          |[0.8208333333333334,0.1791666666666667]   |[0.9780979320991816,0.02190206790081839]  |0.0           |\n",
      "|0.0          |[0.9623459493589778,0.03765405064102219]  |[0.9841285143768546,0.015871485623145443] |0.0           |\n",
      "|0.0          |[0.9830702163416809,0.01692978365831905]  |[0.9839617113281051,0.01603828867189494]  |0.0           |\n",
      "|0.0          |[0.9775131456696741,0.022486854330325975] |[0.9787944162996197,0.02120558370038028]  |0.0           |\n",
      "+-------------+------------------------------------------+------------------------------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.select('rf_prediction',\n",
    "#              'rf_rawPrediction',\n",
    "             'rf_probability',\n",
    "#              'rawPrediction',\n",
    "             'probability','gbt_prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:10:09.658683Z",
     "start_time": "2019-04-11T11:10:06.056239Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Can't extract value from probability#1574: need struct type but got vector;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o530.filter.\n: org.apache.spark.sql.AnalysisException: Can't extract value from probability#1574: need struct type but got vector;\r\n\tat org.apache.spark.sql.catalyst.expressions.ExtractValue$.apply(complexTypeExtractors.scala:73)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:833)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:834)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:834)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:834)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:895)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:895)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:895)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:837)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:837)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:690)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:172)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:178)\r\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3306)\r\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1463)\r\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1477)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-98d4d6be9f69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrfc_preds3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'probability[1] > 0.5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rawPrediction'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'probability'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'prediction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, condition)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \"\"\"\n\u001b[0;32m   1239\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1240\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1241\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"Can't extract value from probability#1574: need struct type but got vector;\""
     ]
    }
   ],
   "source": [
    "rfc_preds3.where('probability[1] > 0.5').select('rawPrediction','probability','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:01:19.925461Z",
     "start_time": "2019-04-11T11:01:16.416283Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+--------------------+--------+----------+\n",
      "|amt|            mnhtn2|        distandtime1|           amt_yrAvg|is_fraud|prediction|\n",
      "+---+------------------+--------------------+--------------------+--------+----------+\n",
      "|  6|0.6004767007199595|7.269931350297886E-5| 0.06976744186046512|       0|       0.0|\n",
      "|  6| 5.278624833901226| 0.04529064636551888|0.047619047619047616|       0|       0.0|\n",
      "|  6| 2.817629414586735|0.020340957367793348| 0.06451612903225806|       0|       0.0|\n",
      "|  7|1.8321572296679909|0.003318644453100983| 0.08641975308641975|       0|       0.0|\n",
      "|  7| 5.508265991381722| 0.04793965179618557| 0.07216494845360824|       0|       0.0|\n",
      "|  8|0.5392933951108095|7.890601175349792E-5| 0.06722689075630252|       0|       0.0|\n",
      "|  8| 4.058025248979389|  0.0861576485982885| 0.07079646017699115|       0|       0.0|\n",
      "|  9| 3.776701133946403| 0.12555522386789905|               0.125|       0|       0.0|\n",
      "|  9|1.4197449184394146| 0.17271835990747136|  0.0891089108910891|       0|       0.0|\n",
      "|  9| 2.314071472257894|0.001530551531997...| 0.04918032786885246|       0|       0.0|\n",
      "|  9|1.3305097395215042|0.012510669859158479| 0.09574468085106383|       0|       0.0|\n",
      "| 10|3.0342674891167842|0.003683168033207235| 0.10416666666666667|       0|       0.0|\n",
      "| 10| 1.607151293804825|0.002260505075889...| 0.11904761904761904|       0|       0.0|\n",
      "| 10|1.8371085927863857|0.001185292526573...| 0.10638297872340426|       0|       0.0|\n",
      "| 12| 2.501658977353707|0.023041899026929234| 0.14457831325301204|       0|       0.0|\n",
      "| 12|  2.55325483866839| 0.08859315887121409| 0.09523809523809523|       0|       0.0|\n",
      "| 13|1.1647618555726134|0.002010810281523...|  0.1566265060240964|       0|       0.0|\n",
      "| 13|1.0398848087134676|0.001394096965778...| 0.12871287128712872|       0|       0.0|\n",
      "| 14|2.5777875462136546|0.017857897791573638| 0.15730337078651685|       0|       0.0|\n",
      "| 14|4.2945999581484084|0.009339538434091748|                 0.2|       0|       0.0|\n",
      "+---+------------------+--------------------+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_preds3.select('amt','mnhtn2','distandtime1','amt_yrAvg','is_fraud','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995282126010989\n"
     ]
    }
   ],
   "source": [
    "my_binary_eval = BinaryClassificationEvaluator(labelCol='is_fraud')\n",
    "print(my_binary_eval.evaluate(rfc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 170\n",
      "True Negatives: 6058\n",
      "False Positives: 19\n",
      "False Negatives: 3\n",
      "Total 6250\n"
     ]
    }
   ],
   "source": [
    "tp = rfc_preds[(rfc_preds.is_fraud == 1) & (rfc_preds.prediction == 1)].count()\n",
    "tn = rfc_preds[(rfc_preds.is_fraud == 0) & (rfc_preds.prediction == 0)].count()\n",
    "fp = rfc_preds[(rfc_preds.is_fraud == 0) & (rfc_preds.prediction == 1)].count()\n",
    "fn = rfc_preds[(rfc_preds.is_fraud == 1) & (rfc_preds.prediction == 0)].count()\n",
    "print (\"True Positives:\", tp)\n",
    "print (\"True Negatives:\", tn)\n",
    "print (\"False Positives:\", fp)\n",
    "print (\"False Negatives:\", fn)\n",
    "print (\"Total\", rfc_preds.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 156\n",
      "True Negatives: 6065\n",
      "False Positives: 5\n",
      "False Negatives: 19\n",
      "Total 6245\n"
     ]
    }
   ],
   "source": [
    "tp = gbt_preds[(gbt_preds.is_fraud == 1) & (gbt_preds.prediction == 1)].count()\n",
    "tn = gbt_preds[(gbt_preds.is_fraud == 0) & (gbt_preds.prediction == 0)].count()\n",
    "fp = gbt_preds[(gbt_preds.is_fraud == 0) & (gbt_preds.prediction == 1)].count()\n",
    "fn = gbt_preds[(gbt_preds.is_fraud == 1) & (gbt_preds.prediction == 0)].count()\n",
    "print (\"True Positives:\", tp)\n",
    "print (\"True Negatives:\", tn)\n",
    "print (\"False Positives:\", fp)\n",
    "print (\"False Negatives:\", fn)\n",
    "print (\"Total\", gbt_preds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Saving & Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rfc_model.write().overwrite().save(\"models/rfc_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.RandomForestClassificationModel"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rfc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed string column 'category' to indexed column 'categoryIndex'\n",
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "StringIndexer will store labels in output column metadata\n",
      "\n",
      "Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n",
      "+---+-------------+----------------+\n",
      "| id|categoryIndex|originalCategory|\n",
      "+---+-------------+----------------+\n",
      "|  0|          0.0|               a|\n",
      "|  1|          2.0|               b|\n",
      "|  2|          1.0|               c|\n",
      "|  3|          0.0|               a|\n",
      "|  4|          0.0|               a|\n",
      "|  5|          1.0|               c|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "print(\"Transformed string column '%s' to indexed column '%s'\"\n",
    "      % (indexer.getInputCol(), indexer.getOutputCol()))\n",
    "indexed.show()\n",
    "\n",
    "print(\"StringIndexer will store labels in output column metadata\\n\")\n",
    "\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
    "converted = converter.transform(indexed)\n",
    "\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
    "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
    "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
