{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:29:00.576970Z",
     "start_time": "2019-04-03T12:29:00.569952Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:30:15.084471Z",
     "start_time": "2019-04-03T12:30:15.078454Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:29:09.273919Z",
     "start_time": "2019-04-03T12:29:01.843977Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:29:27.412119Z",
     "start_time": "2019-04-03T12:29:17.171329Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data = spark.read.csv(\"New_Aggregated_data_final.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:29:27.920540Z",
     "start_time": "2019-04-03T12:29:27.412119Z"
    }
   },
   "outputs": [],
   "source": [
    "customer_data = spark.read.csv(\"Customer_data1.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:29:28.247399Z",
     "start_time": "2019-04-03T12:29:27.923549Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data = trans_data.withColumn(\"sum_prev_day_onl\", trans_data[\"sum_prev_day_onl\"].cast(\"integer\"))\n",
    "trans_data = trans_data.withColumn(\"sum_prev_day_mon_onl\", trans_data[\"sum_prev_day_mon_onl\"].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:29:28.601331Z",
     "start_time": "2019-04-03T12:29:28.250408Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data1 = trans_data.select('_c0','amt', 'Balance',\n",
    "      'sum_prev_day', 'cnt_prev_day_onl', 'sum_prev_day_onl',\n",
    "       '24hrsAvg','qtrAvg','wkAvg', 'monAvg','yrAvg').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:29:28.608348Z",
     "start_time": "2019-04-03T12:29:28.603336Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# train.where(col('cc_num').isNull()).count()\n",
    "# df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "# customer_data.select([count(when(col(c).isNull(),c)).alias(c) for c in customer_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:29:28.625394Z",
     "start_time": "2019-04-03T12:29:28.610354Z"
    }
   },
   "outputs": [],
   "source": [
    "# # merging the data together by their unique \"id\"\n",
    "# train = trans_data.join(customer_data,how='left',on='cc_num')\n",
    "# # all_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:29:28.760749Z",
     "start_time": "2019-04-03T12:29:28.628402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amt',\n",
       " 'Balance',\n",
       " 'sum_prev_day',\n",
       " 'cnt_prev_day_onl',\n",
       " 'sum_prev_day_onl',\n",
       " '24hrsAvg',\n",
       " 'qtrAvg',\n",
       " 'wkAvg',\n",
       " 'monAvg',\n",
       " 'yrAvg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a single vector column\n",
    "cols = trans_data1.drop('_c0').columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:30:24.304325Z",
     "start_time": "2019-04-03T12:30:23.378001Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------+\n",
      "|_c0|features                                                     |\n",
      "+---+-------------------------------------------------------------+\n",
      "|0  |(10,[0,1],[95.0,942.0])                                      |\n",
      "|1  |(10,[0],[90.0])                                              |\n",
      "|2  |(10,[0,1],[188.0,6746.0])                                    |\n",
      "|3  |[100.0,133.0,373.0,0.0,0.0,124.33,124.33,124.33,124.33,124.0]|\n",
      "|4  |[79.0,3115.0,100.0,0.0,0.0,100.0,118.25,118.25,118.25,118.0] |\n",
      "+---+-------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "output_dat = assembler.transform(trans_data1).select('_c0','features')\n",
    "output_dat.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:30:26.290011Z",
     "start_time": "2019-04-03T12:30:24.306328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0|scaledFeatures                                                                                                                                                                                                |\n",
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |[-0.15324509141918674,-0.2082417185038452,-0.7332778530194066,-0.3436387790372,-0.21640155213233406,-0.6804918641670585,-1.2617558227807781,-1.150575009187998,-1.2597350985596223,-1.2583192791022992]       |\n",
      "|1  |[-0.17600379267423114,-0.49825834867770463,-0.7332778530194066,-0.3436387790372,-0.21640155213233406,-0.6804918641670585,-1.2617558227807781,-1.150575009187998,-1.2597350985596223,-1.2583192791022992]      |\n",
      "|2  |[0.270066751924639,1.578654801166091,-0.7332778530194066,-0.3436387790372,-0.21640155213233406,-0.6804918641670585,-1.2617558227807781,-1.150575009187998,-1.2597350985596223,-1.2583192791022992]            |\n",
      "|3  |[-0.13048639016414235,-0.45731120237927225,-0.2293178275752493,-0.3436387790372,-0.21640155213233406,0.02524154037938805,-0.03522609009560643,-0.017476879756091347,-0.04161081398988625,-0.03469200946200004]|\n",
      "|4  |[-0.2260729354353288,0.46076691989084323,-0.5981679266268711,-0.3436387790372,-0.21640155213233406,-0.1128626479308753,-0.09520598854912402,-0.07288777517108364,-0.10117966825019345,-0.09389978057362743]   |\n",
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Center and scale data\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(output_dat)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(output_dat)\n",
    "scaledData.select(['_c0','scaledFeatures']).show(5, truncate=False) #sample centered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:39:45.915966Z",
     "start_time": "2019-04-03T12:39:44.817365Z"
    }
   },
   "outputs": [],
   "source": [
    "#apply PCA\n",
    "pca = PCA(k=8, inputCol=scaler.getOutputCol(), outputCol='pcaFeatures')\n",
    "\n",
    "model = pca.fit(scaledData)\n",
    "transformed_feature = model.transform(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T10:26:16.495641Z",
     "start_time": "2019-04-04T10:26:16.476589Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cov() missing 2 required positional arguments: 'col1' and 'col2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-2ef52bb9ce9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscaledData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: cov() missing 2 required positional arguments: 'col1' and 'col2'"
     ]
    }
   ],
   "source": [
    "scaler.getOutputCol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:39:45.923160Z",
     "start_time": "2019-04-03T12:39:45.919151Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "# pca_features = scaledData.select(\"scaledFeatures\").rdd.map(lambda row : row[0])\n",
    "# mat = RowMatrix(pca_features)\n",
    "# svd = mat.computeSVD(5,True)\n",
    "# svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:39:46.072551Z",
     "start_time": "2019-04-03T12:39:46.056509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53.7645])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of variance explained by each PC\n",
    "np.round(100.00*model.explainedVariance.toArray(),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:38:37.101602Z",
     "start_time": "2019-04-03T12:38:37.084557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1963,  0.2095, -0.0126, -0.7499,  0.5859,  0.0489, -0.0032,\n",
       "        -0.0969],\n",
       "       [-0.0012, -0.0318,  0.9992, -0.0167,  0.0091, -0.0012,  0.0048,\n",
       "        -0.0127],\n",
       "       [-0.3   ,  0.4045,  0.0115,  0.4062,  0.1406,  0.4948,  0.0026,\n",
       "        -0.5627],\n",
       "       [-0.2639,  0.4167,  0.0089, -0.2755, -0.6029, -0.4606,  0.0824,\n",
       "        -0.3136],\n",
       "       [-0.3331,  0.4029,  0.0238, -0.0495, -0.2335,  0.3855, -0.1155,\n",
       "         0.7121],\n",
       "       [-0.3105,  0.241 ,  0.0118,  0.4313,  0.4535, -0.5859,  0.2194,\n",
       "         0.2495],\n",
       "       [-0.3878, -0.3387, -0.0131, -0.0391, -0.065 ,  0.0794,  0.2516,\n",
       "        -0.0115],\n",
       "       [-0.3784, -0.2318, -0.0041,  0.0623,  0.034 , -0.1801, -0.8713,\n",
       "        -0.0737],\n",
       "       [-0.3879, -0.3381, -0.0109, -0.0319, -0.061 ,  0.0607,  0.212 ,\n",
       "        -0.0211],\n",
       "       [-0.3878, -0.3386, -0.0136, -0.0398, -0.0648,  0.0796,  0.2537,\n",
       "        -0.009 ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute loadings of each feature\n",
    "pcs = np.round(model.pc.toArray(),4)\n",
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:38:37.151737Z",
     "start_time": "2019-04-03T12:38:37.112630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>amt</th>\n",
       "      <td>-0.1963</td>\n",
       "      <td>0.2095</td>\n",
       "      <td>-0.0126</td>\n",
       "      <td>-0.7499</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balance</th>\n",
       "      <td>-0.0012</td>\n",
       "      <td>-0.0318</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>-0.0012</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>-0.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_prev_day</th>\n",
       "      <td>-0.3000</td>\n",
       "      <td>0.4045</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.4948</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>-0.5627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnt_prev_day_onl</th>\n",
       "      <td>-0.2639</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>-0.2755</td>\n",
       "      <td>-0.6029</td>\n",
       "      <td>-0.4606</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>-0.3136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_prev_day_onl</th>\n",
       "      <td>-0.3331</td>\n",
       "      <td>0.4029</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>-0.0495</td>\n",
       "      <td>-0.2335</td>\n",
       "      <td>0.3855</td>\n",
       "      <td>-0.1155</td>\n",
       "      <td>0.7121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24hrsAvg</th>\n",
       "      <td>-0.3105</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.4313</td>\n",
       "      <td>0.4535</td>\n",
       "      <td>-0.5859</td>\n",
       "      <td>0.2194</td>\n",
       "      <td>0.2495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qtrAvg</th>\n",
       "      <td>-0.3878</td>\n",
       "      <td>-0.3387</td>\n",
       "      <td>-0.0131</td>\n",
       "      <td>-0.0391</td>\n",
       "      <td>-0.0650</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.2516</td>\n",
       "      <td>-0.0115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wkAvg</th>\n",
       "      <td>-0.3784</td>\n",
       "      <td>-0.2318</td>\n",
       "      <td>-0.0041</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>-0.1801</td>\n",
       "      <td>-0.8713</td>\n",
       "      <td>-0.0737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monAvg</th>\n",
       "      <td>-0.3879</td>\n",
       "      <td>-0.3381</td>\n",
       "      <td>-0.0109</td>\n",
       "      <td>-0.0319</td>\n",
       "      <td>-0.0610</td>\n",
       "      <td>0.0607</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>-0.0211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yrAvg</th>\n",
       "      <td>-0.3878</td>\n",
       "      <td>-0.3386</td>\n",
       "      <td>-0.0136</td>\n",
       "      <td>-0.0398</td>\n",
       "      <td>-0.0648</td>\n",
       "      <td>0.0796</td>\n",
       "      <td>0.2537</td>\n",
       "      <td>-0.0090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     PC1     PC2     PC3     PC4     PC5     PC6     PC7  \\\n",
       "amt              -0.1963  0.2095 -0.0126 -0.7499  0.5859  0.0489 -0.0032   \n",
       "Balance          -0.0012 -0.0318  0.9992 -0.0167  0.0091 -0.0012  0.0048   \n",
       "sum_prev_day     -0.3000  0.4045  0.0115  0.4062  0.1406  0.4948  0.0026   \n",
       "cnt_prev_day_onl -0.2639  0.4167  0.0089 -0.2755 -0.6029 -0.4606  0.0824   \n",
       "sum_prev_day_onl -0.3331  0.4029  0.0238 -0.0495 -0.2335  0.3855 -0.1155   \n",
       "24hrsAvg         -0.3105  0.2410  0.0118  0.4313  0.4535 -0.5859  0.2194   \n",
       "qtrAvg           -0.3878 -0.3387 -0.0131 -0.0391 -0.0650  0.0794  0.2516   \n",
       "wkAvg            -0.3784 -0.2318 -0.0041  0.0623  0.0340 -0.1801 -0.8713   \n",
       "monAvg           -0.3879 -0.3381 -0.0109 -0.0319 -0.0610  0.0607  0.2120   \n",
       "yrAvg            -0.3878 -0.3386 -0.0136 -0.0398 -0.0648  0.0796  0.2537   \n",
       "\n",
       "                     PC8  \n",
       "amt              -0.0969  \n",
       "Balance          -0.0127  \n",
       "sum_prev_day     -0.5627  \n",
       "cnt_prev_day_onl -0.3136  \n",
       "sum_prev_day_onl  0.7121  \n",
       "24hrsAvg          0.2495  \n",
       "qtrAvg           -0.0115  \n",
       "wkAvg            -0.0737  \n",
       "monAvg           -0.0211  \n",
       "yrAvg            -0.0090  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcs = np.round(model.pc.toArray(),4)\n",
    "df_pc = pd.DataFrame(pcs, columns = ['PC'+str(i) for i in range(1, 9)], index = cols)\n",
    "df_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:30:29.417885Z",
     "start_time": "2019-04-03T12:30:29.407856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amt                -0.1963\n",
       "Balance            -0.0012\n",
       "sum_prev_day       -0.3000\n",
       "cnt_prev_day_onl   -0.2639\n",
       "sum_prev_day_onl   -0.3331\n",
       "24hrsAvg           -0.3105\n",
       "qtrAvg             -0.3878\n",
       "wkAvg              -0.3784\n",
       "monAvg             -0.3879\n",
       "yrAvg              -0.3878\n",
       "Name: PC1, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pc['PC1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:50:16.527062Z",
     "start_time": "2019-04-03T12:50:16.470914Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.context import HiveContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler,PCA,StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.param import Param,Params\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.common import callMLlibFunc, JavaModelWrapper\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.mllib.linalg import _convert_to_vector, Vectors, Matrix, DenseMatrix\n",
    "from pyspark.sql.functions import array, col, explode, struct, lit, udf, sum, when,avg,pow,sqrt,mean,log,desc\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import time, os, sys, json, math\n",
    "import datetime as dt\n",
    "import subprocess\n",
    "import getpass\n",
    "import pdb\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from numpy.linalg import eigh\n",
    "import itertools\n",
    "\n",
    "class princomp:\n",
    "    def __init__(self,n=5,std=True,prefix='pcomp'):\n",
    "        self.n=n\n",
    "        self.std=std\n",
    "        self.prefix=prefix\n",
    "        self.cols = [self.prefix+str(i) for i in list(range(1,self.n+1))]\n",
    "        self.inputcol = \"std_features\"\n",
    "        self.outputcol = \"pca_features\"\n",
    "        self.vars = 'all'\n",
    "        self.model = PCA(k=self.n,inputCol=self.inputcol,outputCol=self.outputcol)\n",
    "        self.components = spark.createDataFrame(pd.DataFrame([0]))\n",
    "        self.projections = spark.createDataFrame(pd.DataFrame([0]))\n",
    "        self.result = spark.createDataFrame(pd.DataFrame([0]))\n",
    "    def helpme(self):\n",
    "        print ('|-- Please note that output_parameters will have null values before calling the fit method')\n",
    "        print ('|-- n : input_parameter : sets the number of principal components, default = 5')\n",
    "        print ('|-- std : input_parameter : True/False value specifying whether to standardize the principal components, default = True')\n",
    "        print ('|-- prefix : input_parameter : string specifying the prefix for columns of principal components , default = pcomp')\n",
    "        print ('|-- vars : input_parameter : list of variable names to be used for principal components, default = all')\n",
    "        print ('|-- components : output_parameter : pandas dataframe of coefficients of different input columns for computing principal components')\n",
    "        print ('|-- result : output_parameter : spark dataframe of original variables joined with projections of principal components')\n",
    "        print ('|-- covariance : output_parameter : Numpy array of the covariance matrix')\n",
    "        print ('|-- eigvals : output_parameter : Numpy array of all eigenvalues')\n",
    "        print ('|-- eigvecs : output_parameter : Numpy array of all eigenvectors')\n",
    "        print ('|-- varianceexplained : output_parameter : variance explained by the n principal components')\n",
    "        print ('|-- outputcompfile(file) : method : outputs the components matrix to the specified file')\n",
    "        print ('|-- fit(inputdf,myfeatures) : method : fit method which computes all output parameters')\n",
    "    # SET methods\n",
    "    def setn(self,val):\n",
    "        self.n = val\n",
    "        self.cols = [self.prefix+str(i) for i in list(range(1,self.n+1))]\n",
    "        self.model = PCA(k=self.n,inputCol=self.inputcol,outputCol=self.outputcol)\n",
    "    def setstd(self,val):\n",
    "        self.std = val\n",
    "    def setprefix(self,val):\n",
    "        self.prefix=val\n",
    "        self.cols = [self.prefix+str(i) for i in list(range(1,self.n+1))]\n",
    "    def setcols(self,val):\n",
    "        self.cols = val\n",
    "    def setinputcol(self,val):\n",
    "        self.inputcol = val\n",
    "        self.model = PCA(k=self.n,inputCol=self.inputcol,outputCol=self.outputcol)\n",
    "    def setoutputcol(self,val):\n",
    "        self.outputcol = val\n",
    "        self.model = PCA(k=self.n,inputCol=self.inputcol,outputCol=self.outputcol)\n",
    "    def setvars(self,val):\n",
    "        self.vars = val\n",
    "    def setmodel(self,val):\n",
    "        self.model = val\n",
    "    def setcomponents(self,val):\n",
    "        self.components=val\n",
    "    def setprojections(self,val):\n",
    "        self.projections=val\n",
    "    def setresult(self,val):\n",
    "        self.result = val\n",
    "    # GET methods\n",
    "    def getn(self):\n",
    "        return self.n\n",
    "    def getstd(self):\n",
    "        return self.std\n",
    "    def getprefix(self):\n",
    "        return self.prefix\n",
    "    def getcols(self):\n",
    "        return self.cols\n",
    "    def getinputcol(self):\n",
    "        return self.inputcol\n",
    "    def getoutputcol(self):\n",
    "        return self.outputcol\n",
    "    def getvars(self):\n",
    "        return self.vars\n",
    "    def getmodel(self):\n",
    "        return self.model\n",
    "    def getcomponents(self):\n",
    "        return self.components\n",
    "    def getprojections(self):\n",
    "        return self.projections\n",
    "    def getresult(self):\n",
    "        return self.result\n",
    "    # CORE methods\n",
    "    def vectorizedf(self,inputdf,vars='all'):\n",
    "        \"\"\"Returns the input spark dataframe with an additional column of dense vector features\"\"\"\n",
    "        if vars=='all':\n",
    "            myfeatures = inputdf.columns\n",
    "        else:\n",
    "            myfeatures=vars\n",
    "\n",
    "        assembler = VectorAssembler(inputCols = myfeatures,outputCol=\"features\")\n",
    "        assembled = assembler.transform(inputdf)\n",
    "\n",
    "        as_dense = udf(\n",
    "            lambda v: DenseVector(v.toArray()) if v is not None else None,\n",
    "            VectorUDT()\n",
    "        )\n",
    "\n",
    "        df_dense = assembled.withColumn(\"features1\", as_dense(assembled.features))\n",
    "        df_dense2 = df_dense.drop(\"features\")\n",
    "        df_dense3 = df_dense2.withColumnRenamed(\"features1\",\"features\")\n",
    "        return df_dense3\n",
    "    def outputcompfile(self,filewlocation):\n",
    "        \"\"\" Outputs the loading of principal components to the file specified\"\"\"\n",
    "        df = self.components\n",
    "        df.to_csv(filewlocation,index=False)\n",
    "        print (\"Component matrix is now available at the location : \"+filewlocation)\n",
    "    def identity(self):\n",
    "        \"\"\" Outputs an identity matrix in the form of features column in a dataframe\"\"\"\n",
    "        iden = np.identity(len(self.vars)).tolist()\n",
    "        rddi = sc.parallelize(iden)\n",
    "        df_identity = rddi.map(lambda line:Row(std_features=Vectors.dense(line))).toDF()\n",
    "        return df_identity\n",
    "    def fit(self,inputdf,myfeatures):\n",
    "        \"\"\" Fits the input dataframe in a PCA model with the given features \"\"\"\n",
    "        start_time = time.time()      # Start Timer\n",
    "        if myfeatures=='all':\n",
    "            self.vars = inputdf.columns\n",
    "        else:\n",
    "            self.vars = myfeatures\n",
    "        # vectorize and scale\n",
    "        df_dense = self.vectorizedf(inputdf,self.vars)\n",
    "        df_normalized = self.scalemeanstd(df_dense)\n",
    "        # Compute covariance matrix, eigenvalues and eigenvectors\n",
    "        dfzeromean = df_normalized.select(self.inputcol)\n",
    "        self.covariance = dfzeromean.map(lambda x:np.outer(x,x)).sum()/dfzeromean.count()\n",
    "        col1 = self.covariance.shape[1]\n",
    "        eigvals,eigvecs = eigh(self.covariance)\n",
    "        inds = np.argsort(eigvals)\n",
    "        self.eigvals = eigvals[inds[-1:-(col1+1):-1]]\n",
    "        self.eigvecs = -1*eigvecs.T[inds[-1:-(col1+1):-1]]\n",
    "        self.varianceexplained = np.sum(self.eigvals[0:self.n])/np.sum(self.eigvals)\n",
    "        # Fit PCA model\n",
    "        model1 = self.model.fit(df_normalized)\n",
    "        df_features = model1.transform(df_normalized)\n",
    "        # Compute components and put in a pandas dataframe\n",
    "        df_identity = self.identity()\n",
    "        components = model1.transform(df_identity)\n",
    "        components = components.withColumnRenamed('pca_features','components')\n",
    "        edf_rdd = components.select(\"components\").rdd.map(lambda x: tuple(x.components.toArray().tolist()))\n",
    "        edf_pandas = edf_rdd.toDF(self.cols).toPandas()\n",
    "        comp_ind = sqlContext.createDataFrame([Row(industries=self.vars)])\n",
    "        comp_ind_pandas = comp_ind.select(explode(comp_ind.industries).alias(\"Variable\")).toPandas()\n",
    "        self.components = pd.concat([comp_ind_pandas,edf_pandas],axis=1)\n",
    "        # Compute and standardize projections if self.std = True\n",
    "        if self.std:\n",
    "            projections1=self.scalemeanstd(df_features,inputcol = \"pca_features\",outputcol = \"projections\")\n",
    "        else:\n",
    "            projections1 = df_features.withColumnRenamed('pca_features','projections')\n",
    "        # Prepare data for output\n",
    "        self.projections = projections1.select('projections')\n",
    "        drop_list = ['features','std_features','pca_features']\n",
    "        projections2 = projections1.select([c for c in projections1.columns if c not in drop_list])\n",
    "        l = ['x.'+c for c in inputdf.columns]\n",
    "        cst = '['+\",\".join(l)+']'\n",
    "        final_df = projections1.rdd.map(lambda x: tuple(eval(cst))+tuple(x.projections.toArray().tolist())).toDF(inputdf.columns+self.cols)\n",
    "        self.result = final_df\n",
    "        print(\"PCA fitting took a total of %s seconds \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T12:50:17.499735Z",
     "start_time": "2019-04-03T12:50:17.252901Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'princomp' object has no attribute 'scalemeanstd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-7b180bb06465>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprincomp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans_data1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# here select_features is the list of columns that you wish to perform PCA on\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputcompfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'components.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meigvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvarianceexplained\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-b00838b82acd>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, inputdf, myfeatures)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;31m# vectorize and scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mdf_dense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizedf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mdf_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalemeanstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_dense\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;31m# Compute covariance matrix, eigenvalues and eigenvectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mdfzeromean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_normalized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'princomp' object has no attribute 'scalemeanstd'"
     ]
    }
   ],
   "source": [
    "fs = princomp(n=15)\n",
    "fs.fit(trans_data1,cols) # here select_features is the list of columns that you wish to perform PCA on\n",
    "fs.outputcompfile('components.csv')\n",
    "print (fs.eigvals)\n",
    "print (fs.varianceexplained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:42.996721Z",
     "start_time": "2019-03-26T13:54:42.990704Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.085998Z",
     "start_time": "2019-03-26T13:54:59.050906Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = train.withColumn(\"Birthdate\",from_unixtime(unix_timestamp(train['dob'], 'MM/dd/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.336656Z",
     "start_time": "2019-03-26T13:54:59.301564Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train=(train.withColumn('Yearofbirth',year(train['Birthdate'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.529162Z",
     "start_time": "2019-03-26T13:54:59.494070Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = train.withColumn(\"trans_date\",from_unixtime(unix_timestamp(train['trans_date'], 'MM/dd/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.983356Z",
     "start_time": "2019-03-26T13:54:59.922195Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train=(train.withColumn('Month',month(train['fulltime'])))\n",
    "train = train.withColumn(\"Time\",hour(train[\"fulltime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:55:00.724302Z",
     "start_time": "2019-03-26T13:55:00.254066Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---------+\n",
      "|today_date|Age|      dob|\n",
      "+----------+---+---------+\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "+----------+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = train.withColumn('today_date',lit(2019))\n",
    "train = train.withColumn('Age',train['today_date']-train['Yearofbirth'])\n",
    "train.select('today_date','Age','dob').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:55:51.517678Z",
     "start_time": "2019-03-26T13:55:48.158873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"false\")\n",
    "a=train.drop('first','last')\n",
    "train2 = a.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:56:58.845504Z",
     "start_time": "2019-03-26T13:56:58.534689Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2[\"fulltimepd\"] =  pd.to_datetime(train2['unix_time'],unit='s')\n",
    "\n",
    "train2[\"Weekday\"] = train2[\"fulltimepd\"].dt.strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:00.004543Z",
     "start_time": "2019-03-26T13:56:59.972459Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#train[\"Time_short_for_grouping\"] = train[\"Time_short_for_grouping\"].astype(int)\n",
    "bins = [2,6,11,18,22]\n",
    "labels = [\"Early Morning\",\"Morning\",\"Afternoon\",\"Evening\"]\n",
    "train2[\"Time of day\"] = pd.cut(train2.Time,bins=bins,labels=labels)\n",
    "train2[\"Time of day\"]=train2[\"Time of day\"].cat.add_categories('Midnight') \n",
    "train2[\"Time of day\"] = train2[\"Time of day\"].fillna('Midnight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:01.047277Z",
     "start_time": "2019-03-26T13:57:00.713401Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2[\"Month\"] = train2[\"fulltimepd\"].dt.strftime(\"%B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:01.254821Z",
     "start_time": "2019-03-26T13:57:01.244795Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def haversine_(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate haversine distance between two co-ordinates\"\"\"\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:09.204661Z",
     "start_time": "2019-03-26T13:57:09.198646Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def manhattan_distance_pd(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate manhatten distance between pick_drop\"\"\"\n",
    "    a = haversine_(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_(lat1, lng1, lat2, lng1)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:13.340502Z",
     "start_time": "2019-03-26T13:57:09.618747Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2['hvsine']= haversine_(train2['lat'].values,\n",
    "                                 train2['long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:53.564944Z",
     "start_time": "2019-03-26T14:14:53.551908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2['mnhtn']= manhattan_distance_pd(train2['lat'].values,\n",
    "                                 train2['long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:54.052223Z",
     "start_time": "2019-03-26T14:14:54.039189Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in arcsin\n"
     ]
    }
   ],
   "source": [
    "train2['hvsine2']= haversine_(train2['prev_lat'].values,\n",
    "                                 train2['prev_long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:54.583620Z",
     "start_time": "2019-03-26T14:14:54.570585Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in arcsin\n"
     ]
    }
   ],
   "source": [
    "train2['mnhtn2']= manhattan_distance_pd(train2['prev_lat'].values,\n",
    "                                 train2['prev_long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:55.356650Z",
     "start_time": "2019-03-26T14:14:55.059869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Distance and time dfference\n",
    "train2['distandtime1'] = train2['mnhtn2']/train2['time_diff_min']\n",
    "# train2['distandtime2'] = train2['hvsine2']/train2['time_diff_min']\n",
    "# train['distandtime3'] = train['bearing2']/train['time_diff_min']\n",
    "\n",
    "#train2['distandtime'] =train2['manhtn']/train2['time_diff_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:55.529104Z",
     "start_time": "2019-03-26T14:14:55.519080Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Amount versus year average\n",
    "train2['amt_yrAvg'] = train2['amt']/train2['yrAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:56.213903Z",
     "start_time": "2019-03-26T14:14:56.123665Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "train3=train2.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:57.139335Z",
     "start_time": "2019-03-26T14:14:57.124295Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainimp_f = train3[['Channel', 'Transaction Type', 'gender', 'amt', 'Balance',\n",
    "       'Month', 'Weekday','Time of day','Age','mnhtn','sum_prev_day', 'cnt_prev_day_onl', 'sum_prev_day_onl',\n",
    "       '24hrsAvg','qtrAvg','wkAvg', 'monAvg','yrAvg','mnhtn2','distandtime1','amt_yrAvg','is_fraud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:57.773000Z",
     "start_time": "2019-03-26T14:14:57.724876Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel                 0\n",
       "Transaction Type        0\n",
       "gender                  0\n",
       "amt                     0\n",
       "Balance                 0\n",
       "Month                   0\n",
       "Weekday                 0\n",
       "Time of day             0\n",
       "Age                     0\n",
       "mnhtn                   0\n",
       "sum_prev_day         1496\n",
       "cnt_prev_day_onl    15761\n",
       "sum_prev_day_onl    17759\n",
       "24hrsAvg             1496\n",
       "qtrAvg                332\n",
       "wkAvg                 332\n",
       "monAvg                332\n",
       "yrAvg                 332\n",
       "mnhtn2                 98\n",
       "distandtime1           98\n",
       "amt_yrAvg             332\n",
       "is_fraud                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimp_f.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:58.316428Z",
     "start_time": "2019-03-26T14:14:58.311414Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "colsna= ['sum_prev_day','cnt_prev_day_onl','sum_prev_day_onl',\n",
    "         '24hrsAvg','qtrAvg','wkAvg','monAvg','yrAvg','mnhtn2','distandtime1','amt_yrAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:59.240858Z",
     "start_time": "2019-03-26T14:14:58.849830Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "for col in colsna:\n",
    "    trainimp_f[col] = trainimp_f[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:15:50.759324Z",
     "start_time": "2019-03-26T14:15:50.705180Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Transaction Type</th>\n",
       "      <th>gender</th>\n",
       "      <th>amt</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Time of day</th>\n",
       "      <th>Age</th>\n",
       "      <th>mnhtn</th>\n",
       "      <th>...</th>\n",
       "      <th>sum_prev_day_onl</th>\n",
       "      <th>24hrsAvg</th>\n",
       "      <th>qtrAvg</th>\n",
       "      <th>wkAvg</th>\n",
       "      <th>monAvg</th>\n",
       "      <th>yrAvg</th>\n",
       "      <th>mnhtn2</th>\n",
       "      <th>distandtime1</th>\n",
       "      <th>amt_yrAvg</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>F</td>\n",
       "      <td>95</td>\n",
       "      <td>942</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Midnight</td>\n",
       "      <td>45</td>\n",
       "      <td>1.688628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>F</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>45</td>\n",
       "      <td>2.709298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.020535</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Web</td>\n",
       "      <td>Online</td>\n",
       "      <td>F</td>\n",
       "      <td>188</td>\n",
       "      <td>6746</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>45</td>\n",
       "      <td>1.880668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.175526</td>\n",
       "      <td>0.022514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mobile</td>\n",
       "      <td>USSD</td>\n",
       "      <td>F</td>\n",
       "      <td>100</td>\n",
       "      <td>133</td>\n",
       "      <td>January</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>45</td>\n",
       "      <td>1.911466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.0</td>\n",
       "      <td>3.791882</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATM</td>\n",
       "      <td>ATM</td>\n",
       "      <td>F</td>\n",
       "      <td>79</td>\n",
       "      <td>3115</td>\n",
       "      <td>January</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>45</td>\n",
       "      <td>1.804176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.529470</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.669492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Channel Transaction Type gender  amt  Balance    Month   Weekday  \\\n",
       "0     POS              POS      F   95      942  January    Friday   \n",
       "1     POS              POS      F   90        0  January    Friday   \n",
       "2     Web           Online      F  188     6746  January    Friday   \n",
       "3  Mobile             USSD      F  100      133  January  Saturday   \n",
       "4     ATM              ATM      F   79     3115  January    Sunday   \n",
       "\n",
       "  Time of day  Age     mnhtn    ...     sum_prev_day_onl  24hrsAvg  qtrAvg  \\\n",
       "0    Midnight   45  1.688628    ...                  0.0      0.00    0.00   \n",
       "1     Evening   45  2.709298    ...                  0.0      0.00    0.00   \n",
       "2     Evening   45  1.880668    ...                  0.0      0.00    0.00   \n",
       "3   Afternoon   45  1.911466    ...                  0.0    124.33  124.33   \n",
       "4     Morning   45  1.804176    ...                  0.0    100.00  118.25   \n",
       "\n",
       "    wkAvg  monAvg  yrAvg    mnhtn2  distandtime1  amt_yrAvg  is_fraud  \n",
       "0    0.00    0.00    0.0  0.000000      0.000000   0.000000         0  \n",
       "1    0.00    0.00    0.0  1.020535      0.001001   0.000000         0  \n",
       "2    0.00    0.00    0.0  2.175526      0.022514   0.000000         0  \n",
       "3  124.33  124.33  124.0  3.791882      0.003087   0.806452         0  \n",
       "4  118.25  118.25  118.0  1.529470      0.001447   0.669492         0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimp_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:16:53.811654Z",
     "start_time": "2019-03-26T14:16:53.807642Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, StringType,\n",
    "                              IntegerType, StructType, DoubleType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:16:54.662895Z",
     "start_time": "2019-03-26T14:16:54.626801Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_schema = StructType([StructField(\"Channel\", StringType(), True),\n",
    "               StructField(\"Transaction Type\", StringType(), True),\n",
    "               StructField(\"gender\", StringType(), True),\n",
    "               StructField(\"amt\", IntegerType(), True),\n",
    "               StructField(\"Balance\", IntegerType(), True),\n",
    "               StructField(\"Month\", StringType(), True),\n",
    "               StructField(\"Weekday\", StringType(), True),\n",
    "               StructField(\"Time of day\", StringType(), True),\n",
    "               StructField(\"Age\", IntegerType(), True),\n",
    "               StructField(\"mnhtn\",  DoubleType(), True),\n",
    "               StructField(\"sum_prev_day\", DoubleType(), True),\n",
    "               StructField(\"sum_prev_day_onl\", DoubleType(), True),\n",
    "               StructField(\"cnt_prev_day_onl\", DoubleType(), True),\n",
    "               StructField(\"24hrsAvg\", DoubleType(), True),\n",
    "               StructField(\"wkAvg\", DoubleType(), True),\n",
    "               StructField(\"monAvg\", DoubleType(), True),\n",
    "               StructField(\"qtrAvg\", DoubleType(), True),\n",
    "               StructField(\"yrAvg\", DoubleType(), True),\n",
    "               StructField(\"mnhtn2\", DoubleType(), True),\n",
    "               StructField(\"distandtime1\", DoubleType(), True),\n",
    "               StructField(\"amt_yrAvg\", DoubleType(), True),\n",
    "               StructField(\"is_fraud\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:21:38.599671Z",
     "start_time": "2019-03-26T14:21:37.412548Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Convert\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"false\")\n",
    "#from pyspark.sql.types import *\n",
    "#from pyspark.sql import SqlContext\n",
    "trainimp_f = spark.createDataFrame(trainimp_f,schema=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:21:44.399907Z",
     "start_time": "2019-03-26T14:21:43.251890Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MLlib Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:51:23.301547Z",
     "start_time": "2019-03-26T14:50:43.764116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import (RandomForestClassifier,\n",
    "                                       GBTClassifier)\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, \n",
    "                          outputCol=column+\"_index\",\n",
    "                          handleInvalid=\"keep\").fit(trainimp_f) \n",
    "            for column in list(['Transaction Type',\n",
    "                         'gender',\n",
    "                        \"Time of day\"]) ]\n",
    "\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols= ['amt',\n",
    "                                        \"Age\",\n",
    "                                        \"Time of day_index\",\n",
    "                                        'amt_yrAvg','mnhtn2','distandtime1'], \n",
    "                            outputCol='features')\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(labelCol='is_fraud',\n",
    "                             featuresCol= 'features',\n",
    "                             maxDepth = 11,numTrees=40,seed=1)    \n",
    "\n",
    "pipeline = Pipeline(stages=indexers+[assembler,rfc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:06:05.343015Z",
     "start_time": "2019-03-26T15:06:05.260798Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data,test_data = trainimp_f.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:07:23.593164Z",
     "start_time": "2019-03-26T15:06:06.636451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rfc_model2 = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:17:03.102435Z",
     "start_time": "2019-03-26T15:17:02.829710Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rfc_preds3 = rfc_model2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:17:36.370916Z",
     "start_time": "2019-03-26T15:17:32.962851Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+--------------------+--------+----------+\n",
      "|amt|             mnhtn2|        distandtime1|           amt_yrAvg|is_fraud|prediction|\n",
      "+---+-------------------+--------------------+--------------------+--------+----------+\n",
      "|  6| 0.6004767007199595|7.269931350297886E-5| 0.06976744186046512|       0|       0.0|\n",
      "|  6|  5.278624833901226| 0.04529064636551888|0.047619047619047616|       0|       0.0|\n",
      "|  6|0.21698341972920043|0.007215943456242116| 0.07407407407407407|       0|       0.0|\n",
      "|  6|  2.817629414586735|0.020340957367793348| 0.06451612903225806|       0|       0.0|\n",
      "|  7| 0.6871846682760476|0.002072017694183771| 0.07865168539325842|       0|       0.0|\n",
      "|  7| 1.3819106657909628|0.002747992892521...| 0.07777777777777778|       0|       0.0|\n",
      "|  7|   3.03338696845326|0.003177818834480394| 0.12727272727272726|       0|       0.0|\n",
      "|  8|   2.28200209680964|0.018108253426516747| 0.07920792079207921|       0|       0.0|\n",
      "|  8|  1.996620501516868|0.024142932303710618| 0.08421052631578947|       0|       0.0|\n",
      "|  8|  2.927938207222214|0.013667265122635552| 0.03755868544600939|       0|       0.0|\n",
      "|  9|  3.776701133946403| 0.12555522386789905|               0.125|       0|       0.0|\n",
      "|  9| 1.4197449184394146| 0.17271835990747136|  0.0891089108910891|       0|       0.0|\n",
      "|  9| 0.5586446235335906|1.358769819364670...| 0.09183673469387756|       0|       0.0|\n",
      "|  9| 0.6513777438956946|8.185708374435371E-4| 0.09574468085106383|       0|       0.0|\n",
      "| 10|  1.607151293804825|0.002260505075889...| 0.11904761904761904|       0|       0.0|\n",
      "| 10| 1.8371085927863857|0.001185292526573...| 0.10638297872340426|       0|       0.0|\n",
      "| 11| 2.0521475670796705|0.001597648516971592| 0.21153846153846154|       0|       0.0|\n",
      "| 11|  2.992614292679944| 0.07485278370885304| 0.14864864864864866|       0|       0.0|\n",
      "| 11|   6.62659210532812| 0.03657059660777109| 0.04044117647058824|       0|       0.0|\n",
      "| 12|  2.501658977353707|0.023041899026929234| 0.14457831325301204|       0|       0.0|\n",
      "+---+-------------------+--------------------+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_preds3.select('amt','mnhtn2','distandtime1','amt_yrAvg','is_fraud','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995282126010989\n"
     ]
    }
   ],
   "source": [
    "my_binary_eval = BinaryClassificationEvaluator(labelCol='is_fraud')\n",
    "print(my_binary_eval.evaluate(rfc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 170\n",
      "True Negatives: 6058\n",
      "False Positives: 19\n",
      "False Negatives: 3\n",
      "Total 6250\n"
     ]
    }
   ],
   "source": [
    "tp = rfc_preds[(rfc_preds.is_fraud == 1) & (rfc_preds.prediction == 1)].count()\n",
    "tn = rfc_preds[(rfc_preds.is_fraud == 0) & (rfc_preds.prediction == 0)].count()\n",
    "fp = rfc_preds[(rfc_preds.is_fraud == 0) & (rfc_preds.prediction == 1)].count()\n",
    "fn = rfc_preds[(rfc_preds.is_fraud == 1) & (rfc_preds.prediction == 0)].count()\n",
    "print (\"True Positives:\", tp)\n",
    "print (\"True Negatives:\", tn)\n",
    "print (\"False Positives:\", fp)\n",
    "print (\"False Negatives:\", fn)\n",
    "print (\"Total\", rfc_preds.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 156\n",
      "True Negatives: 6065\n",
      "False Positives: 5\n",
      "False Negatives: 19\n",
      "Total 6245\n"
     ]
    }
   ],
   "source": [
    "tp = gbt_preds[(gbt_preds.is_fraud == 1) & (gbt_preds.prediction == 1)].count()\n",
    "tn = gbt_preds[(gbt_preds.is_fraud == 0) & (gbt_preds.prediction == 0)].count()\n",
    "fp = gbt_preds[(gbt_preds.is_fraud == 0) & (gbt_preds.prediction == 1)].count()\n",
    "fn = gbt_preds[(gbt_preds.is_fraud == 1) & (gbt_preds.prediction == 0)].count()\n",
    "print (\"True Positives:\", tp)\n",
    "print (\"True Negatives:\", tn)\n",
    "print (\"False Positives:\", fp)\n",
    "print (\"False Negatives:\", fn)\n",
    "print (\"Total\", gbt_preds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving & Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model.write().overwrite().save(\"models/rfc_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed string column 'category' to indexed column 'categoryIndex'\n",
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "StringIndexer will store labels in output column metadata\n",
      "\n",
      "Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n",
      "+---+-------------+----------------+\n",
      "| id|categoryIndex|originalCategory|\n",
      "+---+-------------+----------------+\n",
      "|  0|          0.0|               a|\n",
      "|  1|          2.0|               b|\n",
      "|  2|          1.0|               c|\n",
      "|  3|          0.0|               a|\n",
      "|  4|          0.0|               a|\n",
      "|  5|          1.0|               c|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "print(\"Transformed string column '%s' to indexed column '%s'\"\n",
    "      % (indexer.getInputCol(), indexer.getOutputCol()))\n",
    "indexed.show()\n",
    "\n",
    "print(\"StringIndexer will store labels in output column metadata\\n\")\n",
    "\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
    "converted = converter.transform(indexed)\n",
    "\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
    "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
    "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
