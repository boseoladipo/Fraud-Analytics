{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:24:01.144702Z",
     "start_time": "2019-04-01T15:24:01.128661Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:24:05.188332Z",
     "start_time": "2019-04-01T15:24:01.148715Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:24:13.837112Z",
     "start_time": "2019-04-01T15:24:05.190183Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:24:24.778433Z",
     "start_time": "2019-04-01T15:24:13.840125Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data = spark.read.csv(\"New_Aggregated_data_final.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:17.380029Z",
     "start_time": "2019-04-01T15:28:16.691862Z"
    }
   },
   "outputs": [],
   "source": [
    "customer_data = spark.read.csv(\"Customer_data1.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:17.703771Z",
     "start_time": "2019-04-01T15:28:17.383038Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data = trans_data.withColumn(\"sum_prev_day_onl\", trans_data[\"sum_prev_day_onl\"].cast(\"integer\"))\n",
    "trans_data = trans_data.withColumn(\"sum_prev_day_mon_onl\", trans_data[\"sum_prev_day_mon_onl\"].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:18.131893Z",
     "start_time": "2019-04-01T15:28:17.707780Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data1 = trans_data.select('_c0','amt', 'Balance',\n",
    "      'sum_prev_day', 'cnt_prev_day_onl', 'sum_prev_day_onl',\n",
    "       '24hrsAvg','qtrAvg','wkAvg', 'monAvg','yrAvg').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:18.143924Z",
     "start_time": "2019-04-01T15:28:18.135903Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# train.where(col('cc_num').isNull()).count()\n",
    "# df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "# customer_data.select([count(when(col(c).isNull(),c)).alias(c) for c in customer_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:18.164981Z",
     "start_time": "2019-04-01T15:28:18.158963Z"
    }
   },
   "outputs": [],
   "source": [
    "# # merging the data together by their unique \"id\"\n",
    "# train = trans_data.join(customer_data,how='left',on='cc_num')\n",
    "# # all_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:18.306350Z",
     "start_time": "2019-04-01T15:28:18.170996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amt',\n",
       " 'Balance',\n",
       " 'sum_prev_day',\n",
       " 'cnt_prev_day_onl',\n",
       " 'sum_prev_day_onl',\n",
       " '24hrsAvg',\n",
       " 'qtrAvg',\n",
       " 'wkAvg',\n",
       " 'monAvg',\n",
       " 'yrAvg']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a single vector column\n",
    "cols = trans_data1.drop('_c0').columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:19.976728Z",
     "start_time": "2019-04-01T15:28:18.310360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------+\n",
      "|_c0|features                                                     |\n",
      "+---+-------------------------------------------------------------+\n",
      "|0  |(10,[0,1],[95.0,942.0])                                      |\n",
      "|1  |(10,[0],[90.0])                                              |\n",
      "|2  |(10,[0,1],[188.0,6746.0])                                    |\n",
      "|3  |[100.0,133.0,373.0,0.0,0.0,124.33,124.33,124.33,124.33,124.0]|\n",
      "|4  |[79.0,3115.0,100.0,0.0,0.0,100.0,118.25,118.25,118.25,118.0] |\n",
      "+---+-------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "output_dat = assembler.transform(trans_data1).select('_c0','features')\n",
    "output_dat.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:22.524406Z",
     "start_time": "2019-04-01T15:28:19.980739Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0|scaledFeatures                                                                                                                                                                                                |\n",
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |[-0.15324509141918674,-0.2082417185038452,-0.7332778530194066,-0.3436387790372,-0.21640155213233406,-0.6804918641670585,-1.2617558227807781,-1.150575009187998,-1.2597350985596223,-1.2583192791022992]       |\n",
      "|1  |[-0.17600379267423114,-0.49825834867770463,-0.7332778530194066,-0.3436387790372,-0.21640155213233406,-0.6804918641670585,-1.2617558227807781,-1.150575009187998,-1.2597350985596223,-1.2583192791022992]      |\n",
      "|2  |[0.270066751924639,1.578654801166091,-0.7332778530194066,-0.3436387790372,-0.21640155213233406,-0.6804918641670585,-1.2617558227807781,-1.150575009187998,-1.2597350985596223,-1.2583192791022992]            |\n",
      "|3  |[-0.13048639016414235,-0.45731120237927225,-0.2293178275752493,-0.3436387790372,-0.21640155213233406,0.02524154037938805,-0.03522609009560643,-0.017476879756091347,-0.04161081398988625,-0.03469200946200004]|\n",
      "|4  |[-0.2260729354353288,0.46076691989084323,-0.5981679266268711,-0.3436387790372,-0.21640155213233406,-0.1128626479308753,-0.09520598854912402,-0.07288777517108364,-0.10117966825019345,-0.09389978057362743]   |\n",
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Center and scale data\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(output_dat)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(output_dat)\n",
    "scaledData.select(['_c0','scaledFeatures']).show(5, truncate=False) #sample centered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:26.354287Z",
     "start_time": "2019-04-01T15:28:22.531426Z"
    }
   },
   "outputs": [],
   "source": [
    "#apply PCA\n",
    "pca = PCA(k=8, inputCol=scaler.getOutputCol(), outputCol='pcaFeatures')\n",
    "\n",
    "model = pca.fit(scaledData)\n",
    "transformed_feature = model.transform(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T17:32:05.449339Z",
     "start_time": "2019-04-01T17:32:02.223885Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o372.computeSVD.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 25, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\mllib\\linalg\\__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1364)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1364)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1364)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1378)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1377)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:218)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:194)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\mllib\\linalg\\__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1364)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1364)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-98740307576f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpca_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaledData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"scaledFeatures\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRowMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msvd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputeSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msvd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\mllib\\linalg\\distributed.py\u001b[0m in \u001b[0;36mcomputeSVD\u001b[1;34m(self, k, computeU, rCond)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \"\"\"\n\u001b[0;32m    343\u001b[0m         j_model = self._java_matrix_wrapper.call(\n\u001b[1;32m--> 344\u001b[1;33m             \"computeSVD\", int(k), bool(computeU), float(rCond))\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mSingularValueDecomposition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, name, *a)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o372.computeSVD.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 25, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\mllib\\linalg\\__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1364)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1364)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1364)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1378)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1377)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:218)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:194)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\mllib\\linalg\\__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1364)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1364)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "# pca_features = scaledData.select(\"scaledFeatures\").rdd.map(lambda row : row[0])\n",
    "# mat = RowMatrix(pca_features)\n",
    "# svd = mat.computeSVD(5,True)\n",
    "# svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:29.806608Z",
     "start_time": "2019-04-01T15:28:29.785552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53.7645, 15.0258,  9.997 ,  9.0107,  6.3449,  2.2406,  1.8031,\n",
       "        1.7031])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of variance explained by each PC\n",
    "np.round(100.00*model.explainedVariance.toArray(),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:29.870776Z",
     "start_time": "2019-04-01T15:28:29.811622Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1963,  0.2095, -0.0126, -0.7499,  0.5859,  0.0489, -0.0032,\n",
       "        -0.0969],\n",
       "       [-0.0012, -0.0318,  0.9992, -0.0167,  0.0091, -0.0012,  0.0048,\n",
       "        -0.0127],\n",
       "       [-0.3   ,  0.4045,  0.0115,  0.4062,  0.1406,  0.4948,  0.0026,\n",
       "        -0.5627],\n",
       "       [-0.2639,  0.4167,  0.0089, -0.2755, -0.6029, -0.4606,  0.0824,\n",
       "        -0.3136],\n",
       "       [-0.3331,  0.4029,  0.0238, -0.0495, -0.2335,  0.3855, -0.1155,\n",
       "         0.7121],\n",
       "       [-0.3105,  0.241 ,  0.0118,  0.4313,  0.4535, -0.5859,  0.2194,\n",
       "         0.2495],\n",
       "       [-0.3878, -0.3387, -0.0131, -0.0391, -0.065 ,  0.0794,  0.2516,\n",
       "        -0.0115],\n",
       "       [-0.3784, -0.2318, -0.0041,  0.0623,  0.034 , -0.1801, -0.8713,\n",
       "        -0.0737],\n",
       "       [-0.3879, -0.3381, -0.0109, -0.0319, -0.061 ,  0.0607,  0.212 ,\n",
       "        -0.0211],\n",
       "       [-0.3878, -0.3386, -0.0136, -0.0398, -0.0648,  0.0796,  0.2537,\n",
       "        -0.009 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute loadings of each feature\n",
    "pcs = np.round(model.pc.toArray(),4)\n",
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:30.197637Z",
     "start_time": "2019-04-01T15:28:29.874786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>amt</th>\n",
       "      <td>-0.1963</td>\n",
       "      <td>0.2095</td>\n",
       "      <td>-0.0126</td>\n",
       "      <td>-0.7499</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balance</th>\n",
       "      <td>-0.0012</td>\n",
       "      <td>-0.0318</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>-0.0012</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>-0.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_prev_day</th>\n",
       "      <td>-0.3000</td>\n",
       "      <td>0.4045</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.4948</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>-0.5627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnt_prev_day_onl</th>\n",
       "      <td>-0.2639</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>-0.2755</td>\n",
       "      <td>-0.6029</td>\n",
       "      <td>-0.4606</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>-0.3136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_prev_day_onl</th>\n",
       "      <td>-0.3331</td>\n",
       "      <td>0.4029</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>-0.0495</td>\n",
       "      <td>-0.2335</td>\n",
       "      <td>0.3855</td>\n",
       "      <td>-0.1155</td>\n",
       "      <td>0.7121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24hrsAvg</th>\n",
       "      <td>-0.3105</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.4313</td>\n",
       "      <td>0.4535</td>\n",
       "      <td>-0.5859</td>\n",
       "      <td>0.2194</td>\n",
       "      <td>0.2495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qtrAvg</th>\n",
       "      <td>-0.3878</td>\n",
       "      <td>-0.3387</td>\n",
       "      <td>-0.0131</td>\n",
       "      <td>-0.0391</td>\n",
       "      <td>-0.0650</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.2516</td>\n",
       "      <td>-0.0115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wkAvg</th>\n",
       "      <td>-0.3784</td>\n",
       "      <td>-0.2318</td>\n",
       "      <td>-0.0041</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>-0.1801</td>\n",
       "      <td>-0.8713</td>\n",
       "      <td>-0.0737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monAvg</th>\n",
       "      <td>-0.3879</td>\n",
       "      <td>-0.3381</td>\n",
       "      <td>-0.0109</td>\n",
       "      <td>-0.0319</td>\n",
       "      <td>-0.0610</td>\n",
       "      <td>0.0607</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>-0.0211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yrAvg</th>\n",
       "      <td>-0.3878</td>\n",
       "      <td>-0.3386</td>\n",
       "      <td>-0.0136</td>\n",
       "      <td>-0.0398</td>\n",
       "      <td>-0.0648</td>\n",
       "      <td>0.0796</td>\n",
       "      <td>0.2537</td>\n",
       "      <td>-0.0090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     PC1     PC2     PC3     PC4     PC5     PC6     PC7  \\\n",
       "amt              -0.1963  0.2095 -0.0126 -0.7499  0.5859  0.0489 -0.0032   \n",
       "Balance          -0.0012 -0.0318  0.9992 -0.0167  0.0091 -0.0012  0.0048   \n",
       "sum_prev_day     -0.3000  0.4045  0.0115  0.4062  0.1406  0.4948  0.0026   \n",
       "cnt_prev_day_onl -0.2639  0.4167  0.0089 -0.2755 -0.6029 -0.4606  0.0824   \n",
       "sum_prev_day_onl -0.3331  0.4029  0.0238 -0.0495 -0.2335  0.3855 -0.1155   \n",
       "24hrsAvg         -0.3105  0.2410  0.0118  0.4313  0.4535 -0.5859  0.2194   \n",
       "qtrAvg           -0.3878 -0.3387 -0.0131 -0.0391 -0.0650  0.0794  0.2516   \n",
       "wkAvg            -0.3784 -0.2318 -0.0041  0.0623  0.0340 -0.1801 -0.8713   \n",
       "monAvg           -0.3879 -0.3381 -0.0109 -0.0319 -0.0610  0.0607  0.2120   \n",
       "yrAvg            -0.3878 -0.3386 -0.0136 -0.0398 -0.0648  0.0796  0.2537   \n",
       "\n",
       "                     PC8  \n",
       "amt              -0.0969  \n",
       "Balance          -0.0127  \n",
       "sum_prev_day     -0.5627  \n",
       "cnt_prev_day_onl -0.3136  \n",
       "sum_prev_day_onl  0.7121  \n",
       "24hrsAvg          0.2495  \n",
       "qtrAvg           -0.0115  \n",
       "wkAvg            -0.0737  \n",
       "monAvg           -0.0211  \n",
       "yrAvg            -0.0090  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcs = np.round(model.pc.toArray(),4)\n",
    "df_pc = pd.DataFrame(pcs, columns = ['PC'+str(i) for i in range(1, 9)], index = cols)\n",
    "df_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T15:28:30.252777Z",
     "start_time": "2019-04-01T15:28:30.227711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amt                -0.1963\n",
       "Balance            -0.0012\n",
       "sum_prev_day       -0.3000\n",
       "cnt_prev_day_onl   -0.2639\n",
       "sum_prev_day_onl   -0.3331\n",
       "24hrsAvg           -0.3105\n",
       "qtrAvg             -0.3878\n",
       "wkAvg              -0.3784\n",
       "monAvg             -0.3879\n",
       "yrAvg              -0.3878\n",
       "Name: PC1, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pc['PC1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:42.996721Z",
     "start_time": "2019-03-26T13:54:42.990704Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.085998Z",
     "start_time": "2019-03-26T13:54:59.050906Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = train.withColumn(\"Birthdate\",from_unixtime(unix_timestamp(train['dob'], 'MM/dd/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.336656Z",
     "start_time": "2019-03-26T13:54:59.301564Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train=(train.withColumn('Yearofbirth',year(train['Birthdate'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.529162Z",
     "start_time": "2019-03-26T13:54:59.494070Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = train.withColumn(\"trans_date\",from_unixtime(unix_timestamp(train['trans_date'], 'MM/dd/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:54:59.983356Z",
     "start_time": "2019-03-26T13:54:59.922195Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train=(train.withColumn('Month',month(train['fulltime'])))\n",
    "train = train.withColumn(\"Time\",hour(train[\"fulltime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:55:00.724302Z",
     "start_time": "2019-03-26T13:55:00.254066Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---------+\n",
      "|today_date|Age|      dob|\n",
      "+----------+---+---------+\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "+----------+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = train.withColumn('today_date',lit(2019))\n",
    "train = train.withColumn('Age',train['today_date']-train['Yearofbirth'])\n",
    "train.select('today_date','Age','dob').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:55:51.517678Z",
     "start_time": "2019-03-26T13:55:48.158873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"false\")\n",
    "a=train.drop('first','last')\n",
    "train2 = a.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:56:58.845504Z",
     "start_time": "2019-03-26T13:56:58.534689Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2[\"fulltimepd\"] =  pd.to_datetime(train2['unix_time'],unit='s')\n",
    "\n",
    "train2[\"Weekday\"] = train2[\"fulltimepd\"].dt.strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:00.004543Z",
     "start_time": "2019-03-26T13:56:59.972459Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#train[\"Time_short_for_grouping\"] = train[\"Time_short_for_grouping\"].astype(int)\n",
    "bins = [2,6,11,18,22]\n",
    "labels = [\"Early Morning\",\"Morning\",\"Afternoon\",\"Evening\"]\n",
    "train2[\"Time of day\"] = pd.cut(train2.Time,bins=bins,labels=labels)\n",
    "train2[\"Time of day\"]=train2[\"Time of day\"].cat.add_categories('Midnight') \n",
    "train2[\"Time of day\"] = train2[\"Time of day\"].fillna('Midnight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:01.047277Z",
     "start_time": "2019-03-26T13:57:00.713401Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2[\"Month\"] = train2[\"fulltimepd\"].dt.strftime(\"%B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:01.254821Z",
     "start_time": "2019-03-26T13:57:01.244795Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def haversine_(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate haversine distance between two co-ordinates\"\"\"\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:09.204661Z",
     "start_time": "2019-03-26T13:57:09.198646Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def manhattan_distance_pd(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate manhatten distance between pick_drop\"\"\"\n",
    "    a = haversine_(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_(lat1, lng1, lat2, lng1)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T13:57:13.340502Z",
     "start_time": "2019-03-26T13:57:09.618747Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2['hvsine']= haversine_(train2['lat'].values,\n",
    "                                 train2['long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:53.564944Z",
     "start_time": "2019-03-26T14:14:53.551908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2['mnhtn']= manhattan_distance_pd(train2['lat'].values,\n",
    "                                 train2['long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:54.052223Z",
     "start_time": "2019-03-26T14:14:54.039189Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in arcsin\n"
     ]
    }
   ],
   "source": [
    "train2['hvsine2']= haversine_(train2['prev_lat'].values,\n",
    "                                 train2['prev_long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:54.583620Z",
     "start_time": "2019-03-26T14:14:54.570585Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in arcsin\n"
     ]
    }
   ],
   "source": [
    "train2['mnhtn2']= manhattan_distance_pd(train2['prev_lat'].values,\n",
    "                                 train2['prev_long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:55.356650Z",
     "start_time": "2019-03-26T14:14:55.059869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Distance and time dfference\n",
    "train2['distandtime1'] = train2['mnhtn2']/train2['time_diff_min']\n",
    "# train2['distandtime2'] = train2['hvsine2']/train2['time_diff_min']\n",
    "# train['distandtime3'] = train['bearing2']/train['time_diff_min']\n",
    "\n",
    "#train2['distandtime'] =train2['manhtn']/train2['time_diff_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:55.529104Z",
     "start_time": "2019-03-26T14:14:55.519080Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Amount versus year average\n",
    "train2['amt_yrAvg'] = train2['amt']/train2['yrAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:56.213903Z",
     "start_time": "2019-03-26T14:14:56.123665Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "train3=train2.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:57.139335Z",
     "start_time": "2019-03-26T14:14:57.124295Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainimp_f = train3[['Channel', 'Transaction Type', 'gender', 'amt', 'Balance',\n",
    "       'Month', 'Weekday','Time of day','Age','mnhtn','sum_prev_day', 'cnt_prev_day_onl', 'sum_prev_day_onl',\n",
    "       '24hrsAvg','qtrAvg','wkAvg', 'monAvg','yrAvg','mnhtn2','distandtime1','amt_yrAvg','is_fraud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:57.773000Z",
     "start_time": "2019-03-26T14:14:57.724876Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel                 0\n",
       "Transaction Type        0\n",
       "gender                  0\n",
       "amt                     0\n",
       "Balance                 0\n",
       "Month                   0\n",
       "Weekday                 0\n",
       "Time of day             0\n",
       "Age                     0\n",
       "mnhtn                   0\n",
       "sum_prev_day         1496\n",
       "cnt_prev_day_onl    15761\n",
       "sum_prev_day_onl    17759\n",
       "24hrsAvg             1496\n",
       "qtrAvg                332\n",
       "wkAvg                 332\n",
       "monAvg                332\n",
       "yrAvg                 332\n",
       "mnhtn2                 98\n",
       "distandtime1           98\n",
       "amt_yrAvg             332\n",
       "is_fraud                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimp_f.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:58.316428Z",
     "start_time": "2019-03-26T14:14:58.311414Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "colsna= ['sum_prev_day','cnt_prev_day_onl','sum_prev_day_onl',\n",
    "         '24hrsAvg','qtrAvg','wkAvg','monAvg','yrAvg','mnhtn2','distandtime1','amt_yrAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:14:59.240858Z",
     "start_time": "2019-03-26T14:14:58.849830Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "for col in colsna:\n",
    "    trainimp_f[col] = trainimp_f[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:15:50.759324Z",
     "start_time": "2019-03-26T14:15:50.705180Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Transaction Type</th>\n",
       "      <th>gender</th>\n",
       "      <th>amt</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Time of day</th>\n",
       "      <th>Age</th>\n",
       "      <th>mnhtn</th>\n",
       "      <th>...</th>\n",
       "      <th>sum_prev_day_onl</th>\n",
       "      <th>24hrsAvg</th>\n",
       "      <th>qtrAvg</th>\n",
       "      <th>wkAvg</th>\n",
       "      <th>monAvg</th>\n",
       "      <th>yrAvg</th>\n",
       "      <th>mnhtn2</th>\n",
       "      <th>distandtime1</th>\n",
       "      <th>amt_yrAvg</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>F</td>\n",
       "      <td>95</td>\n",
       "      <td>942</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Midnight</td>\n",
       "      <td>45</td>\n",
       "      <td>1.688628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>F</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>45</td>\n",
       "      <td>2.709298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.020535</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Web</td>\n",
       "      <td>Online</td>\n",
       "      <td>F</td>\n",
       "      <td>188</td>\n",
       "      <td>6746</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>45</td>\n",
       "      <td>1.880668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.175526</td>\n",
       "      <td>0.022514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mobile</td>\n",
       "      <td>USSD</td>\n",
       "      <td>F</td>\n",
       "      <td>100</td>\n",
       "      <td>133</td>\n",
       "      <td>January</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>45</td>\n",
       "      <td>1.911466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.0</td>\n",
       "      <td>3.791882</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATM</td>\n",
       "      <td>ATM</td>\n",
       "      <td>F</td>\n",
       "      <td>79</td>\n",
       "      <td>3115</td>\n",
       "      <td>January</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>45</td>\n",
       "      <td>1.804176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.529470</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.669492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Channel Transaction Type gender  amt  Balance    Month   Weekday  \\\n",
       "0     POS              POS      F   95      942  January    Friday   \n",
       "1     POS              POS      F   90        0  January    Friday   \n",
       "2     Web           Online      F  188     6746  January    Friday   \n",
       "3  Mobile             USSD      F  100      133  January  Saturday   \n",
       "4     ATM              ATM      F   79     3115  January    Sunday   \n",
       "\n",
       "  Time of day  Age     mnhtn    ...     sum_prev_day_onl  24hrsAvg  qtrAvg  \\\n",
       "0    Midnight   45  1.688628    ...                  0.0      0.00    0.00   \n",
       "1     Evening   45  2.709298    ...                  0.0      0.00    0.00   \n",
       "2     Evening   45  1.880668    ...                  0.0      0.00    0.00   \n",
       "3   Afternoon   45  1.911466    ...                  0.0    124.33  124.33   \n",
       "4     Morning   45  1.804176    ...                  0.0    100.00  118.25   \n",
       "\n",
       "    wkAvg  monAvg  yrAvg    mnhtn2  distandtime1  amt_yrAvg  is_fraud  \n",
       "0    0.00    0.00    0.0  0.000000      0.000000   0.000000         0  \n",
       "1    0.00    0.00    0.0  1.020535      0.001001   0.000000         0  \n",
       "2    0.00    0.00    0.0  2.175526      0.022514   0.000000         0  \n",
       "3  124.33  124.33  124.0  3.791882      0.003087   0.806452         0  \n",
       "4  118.25  118.25  118.0  1.529470      0.001447   0.669492         0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimp_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:16:53.811654Z",
     "start_time": "2019-03-26T14:16:53.807642Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, StringType,\n",
    "                              IntegerType, StructType, DoubleType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:16:54.662895Z",
     "start_time": "2019-03-26T14:16:54.626801Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_schema = StructType([StructField(\"Channel\", StringType(), True),\n",
    "               StructField(\"Transaction Type\", StringType(), True),\n",
    "               StructField(\"gender\", StringType(), True),\n",
    "               StructField(\"amt\", IntegerType(), True),\n",
    "               StructField(\"Balance\", IntegerType(), True),\n",
    "               StructField(\"Month\", StringType(), True),\n",
    "               StructField(\"Weekday\", StringType(), True),\n",
    "               StructField(\"Time of day\", StringType(), True),\n",
    "               StructField(\"Age\", IntegerType(), True),\n",
    "               StructField(\"mnhtn\",  DoubleType(), True),\n",
    "               StructField(\"sum_prev_day\", DoubleType(), True),\n",
    "               StructField(\"sum_prev_day_onl\", DoubleType(), True),\n",
    "               StructField(\"cnt_prev_day_onl\", DoubleType(), True),\n",
    "               StructField(\"24hrsAvg\", DoubleType(), True),\n",
    "               StructField(\"wkAvg\", DoubleType(), True),\n",
    "               StructField(\"monAvg\", DoubleType(), True),\n",
    "               StructField(\"qtrAvg\", DoubleType(), True),\n",
    "               StructField(\"yrAvg\", DoubleType(), True),\n",
    "               StructField(\"mnhtn2\", DoubleType(), True),\n",
    "               StructField(\"distandtime1\", DoubleType(), True),\n",
    "               StructField(\"amt_yrAvg\", DoubleType(), True),\n",
    "               StructField(\"is_fraud\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:21:38.599671Z",
     "start_time": "2019-03-26T14:21:37.412548Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Convert\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"false\")\n",
    "#from pyspark.sql.types import *\n",
    "#from pyspark.sql import SqlContext\n",
    "trainimp_f = spark.createDataFrame(trainimp_f,schema=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:21:44.399907Z",
     "start_time": "2019-03-26T14:21:43.251890Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MLlib Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:51:23.301547Z",
     "start_time": "2019-03-26T14:50:43.764116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import (RandomForestClassifier,\n",
    "                                       GBTClassifier)\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, \n",
    "                          outputCol=column+\"_index\",\n",
    "                          handleInvalid=\"keep\").fit(trainimp_f) \n",
    "            for column in list(['Transaction Type',\n",
    "                         'gender',\n",
    "                        \"Time of day\"]) ]\n",
    "\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols= ['amt',\n",
    "                                        \"Age\",\n",
    "                                        \"Time of day_index\",\n",
    "                                        'amt_yrAvg','mnhtn2','distandtime1'], \n",
    "                            outputCol='features')\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(labelCol='is_fraud',\n",
    "                             featuresCol= 'features',\n",
    "                             maxDepth = 11,numTrees=40,seed=1)    \n",
    "\n",
    "pipeline = Pipeline(stages=indexers+[assembler,rfc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:06:05.343015Z",
     "start_time": "2019-03-26T15:06:05.260798Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data,test_data = trainimp_f.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:07:23.593164Z",
     "start_time": "2019-03-26T15:06:06.636451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rfc_model2 = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:17:03.102435Z",
     "start_time": "2019-03-26T15:17:02.829710Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rfc_preds3 = rfc_model2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:17:36.370916Z",
     "start_time": "2019-03-26T15:17:32.962851Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+--------------------+--------+----------+\n",
      "|amt|             mnhtn2|        distandtime1|           amt_yrAvg|is_fraud|prediction|\n",
      "+---+-------------------+--------------------+--------------------+--------+----------+\n",
      "|  6| 0.6004767007199595|7.269931350297886E-5| 0.06976744186046512|       0|       0.0|\n",
      "|  6|  5.278624833901226| 0.04529064636551888|0.047619047619047616|       0|       0.0|\n",
      "|  6|0.21698341972920043|0.007215943456242116| 0.07407407407407407|       0|       0.0|\n",
      "|  6|  2.817629414586735|0.020340957367793348| 0.06451612903225806|       0|       0.0|\n",
      "|  7| 0.6871846682760476|0.002072017694183771| 0.07865168539325842|       0|       0.0|\n",
      "|  7| 1.3819106657909628|0.002747992892521...| 0.07777777777777778|       0|       0.0|\n",
      "|  7|   3.03338696845326|0.003177818834480394| 0.12727272727272726|       0|       0.0|\n",
      "|  8|   2.28200209680964|0.018108253426516747| 0.07920792079207921|       0|       0.0|\n",
      "|  8|  1.996620501516868|0.024142932303710618| 0.08421052631578947|       0|       0.0|\n",
      "|  8|  2.927938207222214|0.013667265122635552| 0.03755868544600939|       0|       0.0|\n",
      "|  9|  3.776701133946403| 0.12555522386789905|               0.125|       0|       0.0|\n",
      "|  9| 1.4197449184394146| 0.17271835990747136|  0.0891089108910891|       0|       0.0|\n",
      "|  9| 0.5586446235335906|1.358769819364670...| 0.09183673469387756|       0|       0.0|\n",
      "|  9| 0.6513777438956946|8.185708374435371E-4| 0.09574468085106383|       0|       0.0|\n",
      "| 10|  1.607151293804825|0.002260505075889...| 0.11904761904761904|       0|       0.0|\n",
      "| 10| 1.8371085927863857|0.001185292526573...| 0.10638297872340426|       0|       0.0|\n",
      "| 11| 2.0521475670796705|0.001597648516971592| 0.21153846153846154|       0|       0.0|\n",
      "| 11|  2.992614292679944| 0.07485278370885304| 0.14864864864864866|       0|       0.0|\n",
      "| 11|   6.62659210532812| 0.03657059660777109| 0.04044117647058824|       0|       0.0|\n",
      "| 12|  2.501658977353707|0.023041899026929234| 0.14457831325301204|       0|       0.0|\n",
      "+---+-------------------+--------------------+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_preds3.select('amt','mnhtn2','distandtime1','amt_yrAvg','is_fraud','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995282126010989\n"
     ]
    }
   ],
   "source": [
    "my_binary_eval = BinaryClassificationEvaluator(labelCol='is_fraud')\n",
    "print(my_binary_eval.evaluate(rfc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 170\n",
      "True Negatives: 6058\n",
      "False Positives: 19\n",
      "False Negatives: 3\n",
      "Total 6250\n"
     ]
    }
   ],
   "source": [
    "tp = rfc_preds[(rfc_preds.is_fraud == 1) & (rfc_preds.prediction == 1)].count()\n",
    "tn = rfc_preds[(rfc_preds.is_fraud == 0) & (rfc_preds.prediction == 0)].count()\n",
    "fp = rfc_preds[(rfc_preds.is_fraud == 0) & (rfc_preds.prediction == 1)].count()\n",
    "fn = rfc_preds[(rfc_preds.is_fraud == 1) & (rfc_preds.prediction == 0)].count()\n",
    "print (\"True Positives:\", tp)\n",
    "print (\"True Negatives:\", tn)\n",
    "print (\"False Positives:\", fp)\n",
    "print (\"False Negatives:\", fn)\n",
    "print (\"Total\", rfc_preds.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 156\n",
      "True Negatives: 6065\n",
      "False Positives: 5\n",
      "False Negatives: 19\n",
      "Total 6245\n"
     ]
    }
   ],
   "source": [
    "tp = gbt_preds[(gbt_preds.is_fraud == 1) & (gbt_preds.prediction == 1)].count()\n",
    "tn = gbt_preds[(gbt_preds.is_fraud == 0) & (gbt_preds.prediction == 0)].count()\n",
    "fp = gbt_preds[(gbt_preds.is_fraud == 0) & (gbt_preds.prediction == 1)].count()\n",
    "fn = gbt_preds[(gbt_preds.is_fraud == 1) & (gbt_preds.prediction == 0)].count()\n",
    "print (\"True Positives:\", tp)\n",
    "print (\"True Negatives:\", tn)\n",
    "print (\"False Positives:\", fp)\n",
    "print (\"False Negatives:\", fn)\n",
    "print (\"Total\", gbt_preds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Saving & Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rfc_model.write().overwrite().save(\"models/rfc_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.RandomForestClassificationModel"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rfc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed string column 'category' to indexed column 'categoryIndex'\n",
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "StringIndexer will store labels in output column metadata\n",
      "\n",
      "Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n",
      "+---+-------------+----------------+\n",
      "| id|categoryIndex|originalCategory|\n",
      "+---+-------------+----------------+\n",
      "|  0|          0.0|               a|\n",
      "|  1|          2.0|               b|\n",
      "|  2|          1.0|               c|\n",
      "|  3|          0.0|               a|\n",
      "|  4|          0.0|               a|\n",
      "|  5|          1.0|               c|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "print(\"Transformed string column '%s' to indexed column '%s'\"\n",
    "      % (indexer.getInputCol(), indexer.getOutputCol()))\n",
    "indexed.show()\n",
    "\n",
    "print(\"StringIndexer will store labels in output column metadata\\n\")\n",
    "\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
    "converted = converter.transform(indexed)\n",
    "\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
    "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
    "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
