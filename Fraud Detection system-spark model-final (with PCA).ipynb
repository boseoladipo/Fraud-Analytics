{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:03:54.249421Z",
     "start_time": "2019-04-12T10:03:54.214326Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:05.774072Z",
     "start_time": "2019-04-12T10:03:54.254437Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:24.321400Z",
     "start_time": "2019-04-12T10:04:05.779088Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:44.785827Z",
     "start_time": "2019-04-12T10:04:24.326413Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data = spark.read.csv(\"New_Aggregated_data_final.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:45.657144Z",
     "start_time": "2019-04-12T10:04:44.790840Z"
    }
   },
   "outputs": [],
   "source": [
    "customer_data = spark.read.csv(\"Customer_data1.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:46.228664Z",
     "start_time": "2019-04-12T10:04:45.661154Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data = trans_data.withColumn(\"sum_prev_day_onl\", trans_data[\"sum_prev_day_onl\"].cast(\"integer\"))\n",
    "trans_data = trans_data.withColumn(\"sum_prev_day_mon_onl\", trans_data[\"sum_prev_day_mon_onl\"].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:46.600652Z",
     "start_time": "2019-04-12T10:04:46.232674Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_data1 = trans_data.select('amt', 'Balance',\n",
    "      'sum_prev_day', 'cnt_prev_day_onl', 'sum_prev_day_onl',\n",
    "       '24hrsAvg','qtrAvg','wkAvg', 'monAvg','yrAvg').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:46.779128Z",
     "start_time": "2019-04-12T10:04:46.605667Z"
    }
   },
   "outputs": [],
   "source": [
    "# merging the data together by their unique \"id\"\n",
    "train = trans_data.join(customer_data,how='left',on='cc_num')\n",
    "# all_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:46.790158Z",
     "start_time": "2019-04-12T10:04:46.783139Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# train.where(col('cc_num').isNull()).count()\n",
    "# df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "# customer_data.select([count(when(col(c).isNull(),c)).alias(c) for c in customer_data.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:46.831267Z",
     "start_time": "2019-04-12T10:04:46.796173Z"
    },
    "code_folding": [
     0,
     17
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorizedf(inputdf,vars='all'):\n",
    "        \"\"\"Returns the input spark dataframe with an additional column of dense vector features\"\"\"\n",
    "        if vars=='all':\n",
    "            myfeatures = inputdf.columns\n",
    "        else:\n",
    "            myfeatures=vars\n",
    "        \n",
    "        # assemble features\n",
    "        assembler = VectorAssembler(inputCols = myfeatures,outputCol=\"features\")\n",
    "        assembled = assembler.transform(inputdf).select('features')\n",
    "        \n",
    "        # scale and standardize data\n",
    "        scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
    "        scaled = scaler.fit(assembled).transform(assembled)\n",
    "\n",
    "        return myfeatures, scaled\n",
    "\n",
    "def estimateCovariance(df):\n",
    "    \"\"\"Compute the covariance matrix for a given dataframe.\n",
    "\n",
    "    Note:\n",
    "        The multi-dimensional covariance array should be calculated using outer products.  Don't\n",
    "        forget to normalize the data by first subtracting the mean.\n",
    "\n",
    "    Args:\n",
    "        df:  A Spark dataframe with a column named 'scaledFeatures', which (column) consists of DenseVectors.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the\n",
    "            length of the arrays in the input dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    dfZeroMean = df.select('scaledFeatures')\n",
    "\n",
    "    return dfZeroMean.rdd.map(lambda x: np.outer(x,x)).sum()/df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:46.865356Z",
     "start_time": "2019-04-12T10:04:46.838285Z"
    },
    "code_folding": [
     2
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import eigh\n",
    "\n",
    "def computeEigVals(df):\n",
    "    \"\"\"Computes all eigenvalues.\n",
    "\n",
    "    Note:\n",
    "        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
    "        each eigenvectors as a column. \n",
    "\n",
    "    Args:\n",
    "        df: A Spark dataframe with a 'features' column, which (column) consists of DenseVectors.\n",
    "\n",
    "    Returns:\n",
    "        Eigenvalues is an array of length d (the number of features).\n",
    "     \"\"\"\n",
    "    dfT = vectorizedf(df)[1]\n",
    "    cov = estimateCovariance(dfT)\n",
    "    col = cov.shape[1]\n",
    "    eigVals, eigVecs = eigh(cov)\n",
    "    inds = np.argsort(eigVals)\n",
    "    eigVecs = eigVecs.T[inds[-1:-(col+1):-1]]  \n",
    "    eigVals = eigVals[inds[-1:-(col+1):-1]]  # sort eigenvals\n",
    "\n",
    "    return eigVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:46.904463Z",
     "start_time": "2019-04-12T10:04:46.870371Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def computePCA(df):\n",
    "    \"\"\"\n",
    "    Compute Principal components using pyspark.ml inbuilt function\n",
    "    \n",
    "    Args: \n",
    "        df: A Spark dataframe\n",
    "        k: The number of principal components to be computed which must be less than or equal\n",
    "            to the number of variables in the dataframe\n",
    "            \n",
    "    Returns:\n",
    "        Spark dataframe with contributions of each variable to each principal component. Columns\n",
    "        are principal components, rows are variable names\n",
    "    \n",
    "    \"\"\"\n",
    "    # compute PCA\n",
    "    pca = PCA(k=10, inputCol='scaledFeatures', outputCol='pcaFeatures')\n",
    "    cols, dfT = vectorizedf(df)\n",
    "    model = pca.fit(dfT)\n",
    "    transformed_feature = model.transform(dfT)\n",
    "    \n",
    "    # compute number of significant principal components\n",
    "    totalpercent = np.cumsum(model.explainedVariance.toArray())\n",
    "    numsigpcs = np.where(totalpercent > 0.85)[0][0]\n",
    "    \n",
    "    # create dataframe of mapping\n",
    "    pcs = np.round(model.pc.toArray(),4)[:,:numsigpcs]\n",
    "#     df_pc = pd.DataFrame(pcs, columns = ['PC'+str(i) for i in range(1, len(cols)+1)], index = cols)\n",
    "    \n",
    "    return cols, numsigpcs, pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:46.962616Z",
     "start_time": "2019-04-12T10:04:46.911480Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extractImpVars(n, pcs, eigVals, cols):\n",
    "    \"\"\"\n",
    "    Order the results from the PCA computation to find the variables with the greatest contributions\n",
    "    \n",
    "    Args:\n",
    "        n: number of significant principal components from pca computation (85%)\n",
    "        pcs: computed principal components\n",
    "        eigVals: computed eigenvalues of the principal components\n",
    "        cols: list of columns used to compute  principal components\n",
    "        \n",
    "    Returns:\n",
    "        SigVars: a list of the most significant variables\n",
    "    \"\"\"\n",
    "    contrib = (pcs[:,:n]*eigVals[:n]).sum(axis=1)\n",
    "    \n",
    "    contribdf = pd.DataFrame(contrib, columns = ['PC'], index = cols)\n",
    "    \n",
    "    impVars = contribdf.sort_values(by=['PC'], ascending = False).index[:4].tolist()\n",
    "    \n",
    "    return impVars    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:04:46.992695Z",
     "start_time": "2019-04-12T10:04:46.968631Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pcaColumnSelector(df):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with top columns selected by performing PCA\n",
    "    \"\"\"\n",
    "    eigVals = computeEigVals(trans_data1)\n",
    "    \n",
    "    cols, n, pcs = computePCA(trans_data1)\n",
    "    \n",
    "    impVars = extractImpVars(n, pcs, eigVals, cols)\n",
    "    \n",
    "    return df.select(impVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:13.675662Z",
     "start_time": "2019-04-12T10:04:46.997710Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Balance', 'amt', 'cnt_prev_day_onl', 'sum_prev_day']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcData = pcaColumnSelector(trans_data1)\n",
    "pcData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:13.688699Z",
     "start_time": "2019-04-12T10:05:13.679672Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:13.789966Z",
     "start_time": "2019-04-12T10:05:13.695714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = train.withColumn(\"Birthdate\",from_unixtime(unix_timestamp(train['dob'], 'MM/dd/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:13.860152Z",
     "start_time": "2019-04-12T10:05:13.793976Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train=(train.withColumn('Yearofbirth',year(train['Birthdate'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:13.917304Z",
     "start_time": "2019-04-12T10:05:13.865166Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = train.withColumn(\"trans_date\",from_unixtime(unix_timestamp(train['trans_date'], 'MM/dd/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:14.084750Z",
     "start_time": "2019-04-12T10:05:13.922316Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train=(train.withColumn('Month',month(train['fulltime'])))\n",
    "train = train.withColumn(\"Time\",hour(train[\"fulltime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:15.299982Z",
     "start_time": "2019-04-12T10:05:14.090764Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---------+\n",
      "|today_date|Age|      dob|\n",
      "+----------+---+---------+\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "|      2019| 45|9/23/1974|\n",
      "+----------+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = train.withColumn('today_date',lit(2019))\n",
    "train = train.withColumn('Age',train['today_date']-train['Yearofbirth'])\n",
    "train.select('today_date','Age','dob').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:33.570573Z",
     "start_time": "2019-04-12T10:05:15.304995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"false\")\n",
    "a=train.drop('first','last')\n",
    "train2 = a.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:34.074914Z",
     "start_time": "2019-04-12T10:05:33.575588Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2[\"fulltimepd\"] =  pd.to_datetime(train2['unix_time'],unit='s')\n",
    "\n",
    "train2[\"Weekday\"] = train2[\"fulltimepd\"].dt.strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:34.114021Z",
     "start_time": "2019-04-12T10:05:34.079928Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#train[\"Time_short_for_grouping\"] = train[\"Time_short_for_grouping\"].astype(int)\n",
    "bins = [2,6,11,18,22]\n",
    "labels = [\"Early Morning\",\"Morning\",\"Afternoon\",\"Evening\"]\n",
    "train2[\"Time of day\"] = pd.cut(train2.Time,bins=bins,labels=labels)\n",
    "train2[\"Time of day\"]=train2[\"Time of day\"].cat.add_categories('Midnight') \n",
    "train2[\"Time of day\"] = train2[\"Time of day\"].fillna('Midnight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:34.633399Z",
     "start_time": "2019-04-12T10:05:34.120034Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2[\"Month\"] = train2[\"fulltimepd\"].dt.strftime(\"%B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:34.651450Z",
     "start_time": "2019-04-12T10:05:34.640419Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def haversine_(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate haversine distance between two co-ordinates\"\"\"\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:34.693559Z",
     "start_time": "2019-04-12T10:05:34.656462Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def manhattan_distance_pd(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate manhatten distance between pick_drop\"\"\"\n",
    "    a = haversine_(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_(lat1, lng1, lat2, lng1)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:35.072567Z",
     "start_time": "2019-04-12T10:05:34.699577Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2['hvsine']= haversine_(train2['lat'].values,\n",
    "                                 train2['long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:35.095628Z",
     "start_time": "2019-04-12T10:05:35.076578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train2['mnhtn']= manhattan_distance_pd(train2['lat'].values,\n",
    "                                 train2['long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:35.134732Z",
     "start_time": "2019-04-12T10:05:35.099641Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in arcsin\n"
     ]
    }
   ],
   "source": [
    "train2['hvsine2']= haversine_(train2['prev_lat'].values,\n",
    "                                 train2['prev_long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:35.179854Z",
     "start_time": "2019-04-12T10:05:35.145764Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in arcsin\n"
     ]
    }
   ],
   "source": [
    "train2['mnhtn2']= manhattan_distance_pd(train2['prev_lat'].values,\n",
    "                                 train2['prev_long'].values, train2['merch_lat'].values,\n",
    "                                             train2['merch_long'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:35.584929Z",
     "start_time": "2019-04-12T10:05:35.189879Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Distance and time dfference\n",
    "train2['distandtime1'] = train2['mnhtn2']/train2['time_diff_min']\n",
    "# train2['distandtime2'] = train2['hvsine2']/train2['time_diff_min']\n",
    "# train['distandtime3'] = train['bearing2']/train['time_diff_min']\n",
    "\n",
    "#train2['distandtime'] =train2['manhtn']/train2['time_diff_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:35.605988Z",
     "start_time": "2019-04-12T10:05:35.589943Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Amount versus year average\n",
    "train2['amt_yrAvg'] = train2['amt']/train2['yrAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:35.721293Z",
     "start_time": "2019-04-12T10:05:35.613004Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "train3=train2.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:35.745356Z",
     "start_time": "2019-04-12T10:05:35.728312Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainimp_f = train3[['Channel', 'Transaction Type', 'gender', 'amt', 'Balance',\n",
    "       'Month', 'Weekday','Time of day','Age','mnhtn','sum_prev_day', 'cnt_prev_day_onl', 'sum_prev_day_onl',\n",
    "       '24hrsAvg','qtrAvg','wkAvg', 'monAvg','yrAvg','mnhtn2','distandtime1','amt_yrAvg','is_fraud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:35.831588Z",
     "start_time": "2019-04-12T10:05:35.753379Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel                 0\n",
       "Transaction Type        0\n",
       "gender                  0\n",
       "amt                     0\n",
       "Balance                 0\n",
       "Month                   0\n",
       "Weekday                 0\n",
       "Time of day             0\n",
       "Age                     0\n",
       "mnhtn                   0\n",
       "sum_prev_day         1496\n",
       "cnt_prev_day_onl    15761\n",
       "sum_prev_day_onl    17759\n",
       "24hrsAvg             1496\n",
       "qtrAvg                332\n",
       "wkAvg                 332\n",
       "monAvg                332\n",
       "yrAvg                 332\n",
       "mnhtn2                 98\n",
       "distandtime1           98\n",
       "amt_yrAvg             332\n",
       "is_fraud                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimp_f.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:35.842617Z",
     "start_time": "2019-04-12T10:05:35.836600Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "colsna= ['sum_prev_day','cnt_prev_day_onl','sum_prev_day_onl',\n",
    "         '24hrsAvg','qtrAvg','wkAvg','monAvg','yrAvg','mnhtn2','distandtime1','amt_yrAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:36.416141Z",
     "start_time": "2019-04-12T10:05:35.848633Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoladipo001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "for col in colsna:\n",
    "    trainimp_f[col] = trainimp_f[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:36.742009Z",
     "start_time": "2019-04-12T10:05:36.421155Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Transaction Type</th>\n",
       "      <th>gender</th>\n",
       "      <th>amt</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Time of day</th>\n",
       "      <th>Age</th>\n",
       "      <th>mnhtn</th>\n",
       "      <th>...</th>\n",
       "      <th>sum_prev_day_onl</th>\n",
       "      <th>24hrsAvg</th>\n",
       "      <th>qtrAvg</th>\n",
       "      <th>wkAvg</th>\n",
       "      <th>monAvg</th>\n",
       "      <th>yrAvg</th>\n",
       "      <th>mnhtn2</th>\n",
       "      <th>distandtime1</th>\n",
       "      <th>amt_yrAvg</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>F</td>\n",
       "      <td>95</td>\n",
       "      <td>942</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Midnight</td>\n",
       "      <td>45</td>\n",
       "      <td>1.688628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>F</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>45</td>\n",
       "      <td>2.709298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.020535</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Web</td>\n",
       "      <td>Online</td>\n",
       "      <td>F</td>\n",
       "      <td>188</td>\n",
       "      <td>6746</td>\n",
       "      <td>January</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>45</td>\n",
       "      <td>1.880668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.175526</td>\n",
       "      <td>0.022514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mobile</td>\n",
       "      <td>USSD</td>\n",
       "      <td>F</td>\n",
       "      <td>100</td>\n",
       "      <td>133</td>\n",
       "      <td>January</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>45</td>\n",
       "      <td>1.911466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.33</td>\n",
       "      <td>124.0</td>\n",
       "      <td>3.791882</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATM</td>\n",
       "      <td>ATM</td>\n",
       "      <td>F</td>\n",
       "      <td>79</td>\n",
       "      <td>3115</td>\n",
       "      <td>January</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>45</td>\n",
       "      <td>1.804176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.25</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.529470</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.669492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Channel Transaction Type gender  amt  Balance    Month   Weekday  \\\n",
       "0     POS              POS      F   95      942  January    Friday   \n",
       "1     POS              POS      F   90        0  January    Friday   \n",
       "2     Web           Online      F  188     6746  January    Friday   \n",
       "3  Mobile             USSD      F  100      133  January  Saturday   \n",
       "4     ATM              ATM      F   79     3115  January    Sunday   \n",
       "\n",
       "  Time of day  Age     mnhtn    ...     sum_prev_day_onl  24hrsAvg  qtrAvg  \\\n",
       "0    Midnight   45  1.688628    ...                  0.0      0.00    0.00   \n",
       "1     Evening   45  2.709298    ...                  0.0      0.00    0.00   \n",
       "2     Evening   45  1.880668    ...                  0.0      0.00    0.00   \n",
       "3   Afternoon   45  1.911466    ...                  0.0    124.33  124.33   \n",
       "4     Morning   45  1.804176    ...                  0.0    100.00  118.25   \n",
       "\n",
       "    wkAvg  monAvg  yrAvg    mnhtn2  distandtime1  amt_yrAvg  is_fraud  \n",
       "0    0.00    0.00    0.0  0.000000      0.000000   0.000000         0  \n",
       "1    0.00    0.00    0.0  1.020535      0.001001   0.000000         0  \n",
       "2    0.00    0.00    0.0  2.175526      0.022514   0.000000         0  \n",
       "3  124.33  124.33  124.0  3.791882      0.003087   0.806452         0  \n",
       "4  118.25  118.25  118.0  1.529470      0.001447   0.669492         0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimp_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:36.757047Z",
     "start_time": "2019-04-12T10:05:36.748024Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, StringType,\n",
    "                              IntegerType, StructType, DoubleType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:36.825229Z",
     "start_time": "2019-04-12T10:05:36.772088Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_schema = StructType([StructField(\"Channel\", StringType(), True),\n",
    "               StructField(\"Transaction Type\", StringType(), True),\n",
    "               StructField(\"gender\", StringType(), True),\n",
    "               StructField(\"amt\", IntegerType(), True),\n",
    "               StructField(\"Balance\", IntegerType(), True),\n",
    "               StructField(\"Month\", StringType(), True),\n",
    "               StructField(\"Weekday\", StringType(), True),\n",
    "               StructField(\"Time of day\", StringType(), True),\n",
    "               StructField(\"Age\", IntegerType(), True),\n",
    "               StructField(\"mnhtn\",  DoubleType(), True),\n",
    "               StructField(\"sum_prev_day\", DoubleType(), True),\n",
    "               StructField(\"sum_prev_day_onl\", DoubleType(), True),\n",
    "               StructField(\"cnt_prev_day_onl\", DoubleType(), True),\n",
    "               StructField(\"24hrsAvg\", DoubleType(), True),\n",
    "               StructField(\"wkAvg\", DoubleType(), True),\n",
    "               StructField(\"monAvg\", DoubleType(), True),\n",
    "               StructField(\"qtrAvg\", DoubleType(), True),\n",
    "               StructField(\"yrAvg\", DoubleType(), True),\n",
    "               StructField(\"mnhtn2\", DoubleType(), True),\n",
    "               StructField(\"distandtime1\", DoubleType(), True),\n",
    "               StructField(\"amt_yrAvg\", DoubleType(), True),\n",
    "               StructField(\"is_fraud\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:38.853626Z",
     "start_time": "2019-04-12T10:05:36.834255Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Convert\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"false\")\n",
    "#from pyspark.sql.types import *\n",
    "#from pyspark.sql import SqlContext\n",
    "trainimp_f = spark.createDataFrame(trainimp_f,schema=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:38.862649Z",
     "start_time": "2019-04-12T10:05:38.857635Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLlib Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:38.942873Z",
     "start_time": "2019-04-12T10:05:38.867663Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getFeatures(numericCols, categoricalCols):\n",
    "    \"\"\"\n",
    "    Get dummy variables and concatenate with continous variables\n",
    "    \n",
    "    Args:\n",
    "        df: the dataframe\n",
    "        numericCols: list of columns with numerical data\n",
    "        categoricalCols: list of columns with categorical data\n",
    "        labelCol: target variable column name\n",
    "    \n",
    "    Returns:\n",
    "        featurePipeline: pipeline object to extract features\n",
    "    \n",
    "    \"\"\"\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "    \n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "                    for c in categoricalCols ]\n",
    "    \n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                              outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "                    for indexer in indexers ]\n",
    "    \n",
    "    assembler_indexed = VectorAssembler(inputCols= [indexer.getOutputCol() for indexer in indexers]\n",
    "                                + numericCols, \n",
    "                                outputCol='features_ind')\n",
    "\n",
    "    assembler_encoded = VectorAssembler(inputCols= [encoder.getOutputCol() for encoder in encoders]\n",
    "                                + numericCols, \n",
    "                                outputCol='features_enc')\n",
    "    \n",
    "    featurePipeline = Pipeline(stages=indexers+encoders+[assembler_indexed,assembler_encoded])\n",
    "    \n",
    "    return featurePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:38.978957Z",
     "start_time": "2019-04-12T10:05:38.949878Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getModels(indexedFeatures, encodedFeatures, labelCol):\n",
    "    \"\"\"\n",
    "    Get instances of models for the ensemble model\n",
    "    \n",
    "    Args:\n",
    "        indexedFeatures: name of indexed features column\n",
    "        encodedFeatures: name of encoded features column\n",
    "        labelCol: target variable column name\n",
    "    \"\"\"\n",
    "    \n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "    \n",
    "    rfc = RandomForestClassifier(labelCol=labelCol, predictionCol='rf_prediction', \n",
    "                                 probabilityCol='rf_probability', rawPredictionCol='rf_rawPrediction',\n",
    "                                 featuresCol= indexedFeatures,\n",
    "                                 maxDepth = 11,numTrees=40,seed=1)    \n",
    "\n",
    "    gbt = GBTClassifier(labelCol=labelCol,  featuresCol= indexedFeatures,\n",
    "                        predictionCol='gbt_prediction', maxIter=33, \n",
    "                        maxDepth = 3,stepSize=0.72,seed=1 )\n",
    "\n",
    "    logr = LogisticRegression(featuresCol=encodedFeatures, labelCol=labelCol, predictionCol='logr_prediction', \n",
    "                                 probabilityCol='logr_probability', rawPredictionCol='logr_rawPrediction')\n",
    "\n",
    "    modelPipeline = Pipeline(stages=[rfc,gbt,logr])\n",
    "    \n",
    "    return modelPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:39.013054Z",
     "start_time": "2019-04-12T10:05:38.984972Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "numCols = ['Balance', 'amt', 'cnt_prev_day_onl', 'sum_prev_day']\n",
    "catCols = []\n",
    "\n",
    "# pipeline.fit(train_data).transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:39.454221Z",
     "start_time": "2019-04-12T10:05:39.021069Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "finalPipeline = Pipeline(stages=[getFeatures(numericCols=numCols, categoricalCols=catCols),\n",
    "                                getModels(indexedFeatures='features_ind', encodedFeatures='features_enc',\n",
    "                                         labelCol='is_fraud')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:05:39.516385Z",
     "start_time": "2019-04-12T10:05:39.458232Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data,test_data = trainimp_f.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:08:34.484744Z",
     "start_time": "2019-04-12T10:05:39.520397Z"
    }
   },
   "outputs": [],
   "source": [
    "mod = finalPipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T10:08:34.848712Z",
     "start_time": "2019-04-12T10:08:34.489758Z"
    }
   },
   "outputs": [],
   "source": [
    "predall = mod.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T17:43:15.743324Z",
     "start_time": "2019-04-11T17:43:12.066662Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----------------------------------------+------------------------------------------+-------------+\n",
      "|rf_probability                           |probability                              |logr_probability                          |rf_prediction|\n",
      "+-----------------------------------------+-----------------------------------------+------------------------------------------+-------------+\n",
      "|[0.984370079582342,0.015629920417657994] |[0.9904998854522721,0.009500114547727923]|[0.9821764167999824,0.017823583200017666] |0.0          |\n",
      "|[0.9978892399481978,0.002110760051802235]|[0.9902339846618946,0.009766015338105394]|[0.9976713570549718,0.002328642945028161] |0.0          |\n",
      "|[0.9991794471118405,8.20552888159403E-4] |[0.9935417822374103,0.006458217762589702]|[0.9949527214629974,0.005047278537002549] |0.0          |\n",
      "|[0.988884177108617,0.011115822891382937] |[0.9902339846618946,0.009766015338105394]|[0.9977282693504353,0.0022717306495645957]|0.0          |\n",
      "|[0.999677492653485,3.225073465148879E-4] |[0.9940516919662843,0.00594830803371571] |[0.9998111416709903,1.8885832900967657E-4]|0.0          |\n",
      "+-----------------------------------------+-----------------------------------------+------------------------------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predall.select('rf_probability','probability', 'logr_probability','rf_prediction').show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T14:50:14.771736Z",
     "start_time": "2019-04-12T14:50:06.060568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|prob  |\n",
      "+------+\n",
      "|0.0108|\n",
      "|0.0039|\n",
      "|0.0072|\n",
      "|0.0072|\n",
      "|0.0022|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "@udf(returnType=FloatType())\n",
    "def getavgprob(*args):\n",
    "    \"\"\"\n",
    "    Compute average probability from model output probabilities\n",
    "    \n",
    "    Args:\n",
    "        Names of probability output columns from models\n",
    "    \"\"\"\n",
    "    argList= [float(i[1]) for i in args]\n",
    "    \n",
    "    meanProba = (__builtin__.sum(argList))/len(argList)\n",
    "    \n",
    "    return __builtin__.round(meanProba,4)\n",
    "\n",
    "summPred = predall.withColumn('prob', getallprob('probability','rf_probability','logr_probability'))\n",
    "summPred.select('prob').show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T17:43:47.051996Z",
     "start_time": "2019-04-11T17:43:34.062864Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9328404058860955\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol='is_fraud',\n",
    "                                               rawPredictionCol='rf_rawPrediction')\n",
    "print(my_binary_eval.evaluate(predall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995282126010989\n"
     ]
    }
   ],
   "source": [
    "my_binary_eval = BinaryClassificationEvaluator(labelCol='is_fraud')\n",
    "print(my_binary_eval.evaluate(rfc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 170\n",
      "True Negatives: 6058\n",
      "False Positives: 19\n",
      "False Negatives: 3\n",
      "Total 6250\n"
     ]
    }
   ],
   "source": [
    "tp = rfc_preds[(rfc_preds.is_fraud == 1) & (rfc_preds.prediction == 1)].count()\n",
    "tn = rfc_preds[(rfc_preds.is_fraud == 0) & (rfc_preds.prediction == 0)].count()\n",
    "fp = rfc_preds[(rfc_preds.is_fraud == 0) & (rfc_preds.prediction == 1)].count()\n",
    "fn = rfc_preds[(rfc_preds.is_fraud == 1) & (rfc_preds.prediction == 0)].count()\n",
    "print (\"True Positives:\", tp)\n",
    "print (\"True Negatives:\", tn)\n",
    "print (\"False Positives:\", fp)\n",
    "print (\"False Negatives:\", fn)\n",
    "print (\"Total\", rfc_preds.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 156\n",
      "True Negatives: 6065\n",
      "False Positives: 5\n",
      "False Negatives: 19\n",
      "Total 6245\n"
     ]
    }
   ],
   "source": [
    "tp = gbt_preds[(gbt_preds.is_fraud == 1) & (gbt_preds.prediction == 1)].count()\n",
    "tn = gbt_preds[(gbt_preds.is_fraud == 0) & (gbt_preds.prediction == 0)].count()\n",
    "fp = gbt_preds[(gbt_preds.is_fraud == 0) & (gbt_preds.prediction == 1)].count()\n",
    "fn = gbt_preds[(gbt_preds.is_fraud == 1) & (gbt_preds.prediction == 0)].count()\n",
    "print (\"True Positives:\", tp)\n",
    "print (\"True Negatives:\", tn)\n",
    "print (\"False Positives:\", fp)\n",
    "print (\"False Negatives:\", fn)\n",
    "print (\"Total\", gbt_preds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Saving & Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rfc_model.write().overwrite().save(\"models/rfc_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.RandomForestClassificationModel"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rfc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed string column 'category' to indexed column 'categoryIndex'\n",
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "StringIndexer will store labels in output column metadata\n",
      "\n",
      "Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n",
      "+---+-------------+----------------+\n",
      "| id|categoryIndex|originalCategory|\n",
      "+---+-------------+----------------+\n",
      "|  0|          0.0|               a|\n",
      "|  1|          2.0|               b|\n",
      "|  2|          1.0|               c|\n",
      "|  3|          0.0|               a|\n",
      "|  4|          0.0|               a|\n",
      "|  5|          1.0|               c|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "print(\"Transformed string column '%s' to indexed column '%s'\"\n",
    "      % (indexer.getInputCol(), indexer.getOutputCol()))\n",
    "indexed.show()\n",
    "\n",
    "print(\"StringIndexer will store labels in output column metadata\\n\")\n",
    "\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
    "converted = converter.transform(indexed)\n",
    "\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
    "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
    "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
